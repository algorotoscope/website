<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-01-14T14:11:21-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kin Lane</title><subtitle>The ramblings, thoughts, and channeling of Kin Lane.</subtitle><author><name>Kin Lane</name></author><entry><title type="html">Why I Do Algorotoscope</title><link href="http://localhost:4000/2023/01/14/why-i-do-algorotoscope/" rel="alternate" type="text/html" title="Why I Do Algorotoscope" /><published>2023-01-14T04:00:00-08:00</published><updated>2023-01-14T04:00:00-08:00</updated><id>http://localhost:4000/2023/01/14/why-i-do-algorotoscope</id><content type="html" xml:base="http://localhost:4000/2023/01/14/why-i-do-algorotoscope/">&lt;p&gt;I started Algorotoscope as an escape from the 2016 election. I wanted to learn more about machine learning and TensorFlow while simultaneously wanting to escape the world. Once I established a working model I wanted to get out in the world more to take photos and videos. I lost the Amazon Machine Image (AMI) for my Algorotoscope process as part of switching accounts, so I haven’t produced anything new in a while. I am just working from images I have already applied my models. Now I have my AMI and a newly optimized process for producing texture transfer models, but also applying them to large amounts of images. So, I am looking to continue my exploration, with an emphasis on being out in the world taking photos that speak to the characteristics of each ML model, applying to my new and old images, and telling stories that support them.
&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/bf-skinner-behaviorism/bf-skinner-alan-turing-side.jpg&quot; alt=&quot;BF Skinner Turing&quot; /&gt;
If I have to reaffirm why I do Algorotoscope. It is less about learning ML or escaping the world now. I’d say it is about exploring the world. Finding relevant images I can train models on and then going out in the world to take photographs and film videos that leverage the strengths of each of the models. Visually and contextually. I just don’t want to be a one trick pony with the dark models applied to every day images. I want to come at it from multiple dimensions, reflecting on both the physical and digital spaces we live in. The overlap of the two. The obfuscation. The reality distortion field that consumes us online, but then also colors, paints, and texturizes the actual world around us.
&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/copper-circuit/christianity.jpg&quot; alt=&quot;Polishing Technological Religion&quot; /&gt;
To understand where I am coming from, it helps to understand how I see the world around us. I see APIs. I see millions of digital interfaces distorting our worlds. Some recent examples can be found with Facebook, Twitter, and the 2016 election, or how our legal system is being automated with algorithms Human Resources for our companies are flowing through these pipes, deciding who they hire and who they don’t. I see our world flowing through APIs each day, and I see how existing biases are being codified in these pipes and gears that our powering not just our online lives, but our physical world. This is why I publish Algorotoscope images with each post you see on API Evangelist—it is a visual representation of the space I’m telling a story in. I need a steady stream of images that are relevant to how I am describing the knobs, levers, and gears of the machine, but also with some context of the bias being baked in.
&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/nazi-poster/downtown-usa.jpg&quot; alt=&quot;Small Town Propaganda&quot; /&gt;
Algorotoscope is how I show how I see the digital world consuming our physical worlds. Consuming, then dictating and shaping our lives. I don’t know how else to illustrate it. I am a software engineer, architect, and storyteller. I can’t draw this shit. I can only express it using ML and photographing—two things I cam capable of doing in this moment. The Algorotoscope images are the closest I can come to articulating what I am seeing distort and obfuscate our lives. It is the static in our heads after we’ve been online too long. It visualizes why we have a lack of control over our lives right now. It provides a quick snapshot of just one more moment where a moment of our lives is being reduced to a transaction.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">I started Algorotoscope as an escape from the 2016 election. I wanted to learn more about machine learning and TensorFlow while simultaneously wanting to escape the world. Once I established a working model I wanted to get out in the world more to take photos and videos. I lost the Amazon Machine Image (AMI) for my Algorotoscope process as part of switching accounts, so I haven’t produced anything new in a while. I am just working from images I have already applied my models. Now I have my AMI and a newly optimized process for producing texture transfer models, but also applying them to large amounts of images. So, I am looking to continue my exploration, with an emphasis on being out in the world taking photos that speak to the characteristics of each ML model, applying to my new and old images, and telling stories that support them. If I have to reaffirm why I do Algorotoscope. It is less about learning ML or escaping the world now. I’d say it is about exploring the world. Finding relevant images I can train models on and then going out in the world to take photographs and film videos that leverage the strengths of each of the models. Visually and contextually. I just don’t want to be a one trick pony with the dark models applied to every day images. I want to come at it from multiple dimensions, reflecting on both the physical and digital spaces we live in. The overlap of the two. The obfuscation. The reality distortion field that consumes us online, but then also colors, paints, and texturizes the actual world around us. To understand where I am coming from, it helps to understand how I see the world around us. I see APIs. I see millions of digital interfaces distorting our worlds. Some recent examples can be found with Facebook, Twitter, and the 2016 election, or how our legal system is being automated with algorithms Human Resources for our companies are flowing through these pipes, deciding who they hire and who they don’t. I see our world flowing through APIs each day, and I see how existing biases are being codified in these pipes and gears that our powering not just our online lives, but our physical world. This is why I publish Algorotoscope images with each post you see on API Evangelist—it is a visual representation of the space I’m telling a story in. I need a steady stream of images that are relevant to how I am describing the knobs, levers, and gears of the machine, but also with some context of the bias being baked in. Algorotoscope is how I show how I see the digital world consuming our physical worlds. Consuming, then dictating and shaping our lives. I don’t know how else to illustrate it. I am a software engineer, architect, and storyteller. I can’t draw this shit. I can only express it using ML and photographing—two things I cam capable of doing in this moment. The Algorotoscope images are the closest I can come to articulating what I am seeing distort and obfuscate our lives. It is the static in our heads after we’ve been online too long. It visualizes why we have a lack of control over our lives right now. It provides a quick snapshot of just one more moment where a moment of our lives is being reduced to a transaction.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/copper-circuit-kin-chesapeake.jpg" /></entry><entry><title type="html">Rebuilding the Texture Transfer ML Process</title><link href="http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/" rel="alternate" type="text/html" title="Rebuilding the Texture Transfer ML Process" /><published>2023-01-14T04:00:00-08:00</published><updated>2023-01-14T04:00:00-08:00</updated><id>http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images</id><content type="html" xml:base="http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/">&lt;p&gt;Historically the ML models I have trained for Algorotoscope have been fairly dark in nature. I had the training wheels of the models that came with the ML process that I adopted, but then I quickly began &lt;a href=&quot;https://algorithmic.rotoscope.work/collections/the-persistence-of-memory/&quot;&gt;training on Salvador Dali and playing around with a distortion in time&lt;/a&gt;, but then found myself drowning in Nazi and Russian propaganda. I would take these models and apply them to normal everyday photos I have taken—obfuscating the darkness in the texture applied to each photo. Now, I am flipping the polarity, training more positive models, but then applying them to darker photos, experimenting with how I can further obfuscate the real world around was with digital textures. I have two models to demonstrate how I am seeing things.&lt;/p&gt;

&lt;h2 id=&quot;train-travel-to-california&quot;&gt;Train Travel to California&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/california-travel-by-train/california-travel-by-train.jpeg&quot; alt=&quot;Train Travel to California&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;applied-to-train-travel-in-california&quot;&gt;Applied to Train Travel in California&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/california-travel-by-train/california-travel-by-train-freeway-interchange-fence.jpg&quot; alt=&quot;Applied to Train Travel in California&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;oakland-california-bridge&quot;&gt;Oakland, California Bridge&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california.jpeg&quot; alt=&quot;Oakland, California Bridge&quot; /&gt;￼&lt;/p&gt;
&lt;h2 id=&quot;applied-to-oakland-california-bridge&quot;&gt;Applied to Oakland, California Bridge&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-freeway-camp.jpg&quot; alt=&quot;Applied to Oakland, California Bridge&quot; /&gt;￼&lt;/p&gt;

&lt;p&gt;Early on in my exploration I wanted to take previous propaganda and make pictures distorted in an algorithmic way using this old world images. Now I am feeling like I want to take old world marketing and use it to distort in an algorithmic way the photos I am taking as I walk around my community, state, and wider. As always, I have no idea where I am going with this. I am just exploring machine learning in my own way, taking photographs, and playing around with how I can mix the two to say something.&lt;/p&gt;

&lt;p&gt;I historically have used my images in my storytelling on API Evangelist. Using everyday photos that are obfuscated with some pretty dark propaganda from our pasts to quietly show the distortion field that is being created using APIs. These new round of photos most likely will have no place in my API Evangelist storytelling, pushing me to use on my Alternate channels, Kin Lane, and other places I am expressing myself online. I am happy that I finally have my text transfer ML process back working, and I also now have two more models to play with when it comes to applying to photos. I’ll keep learning where these photos translate best. How the clouds, rocks, buildings, people, and other real world objects get obfuscated, and I’ll work to take photos that bring this out in new and interesting ways.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">Historically the ML models I have trained for Algorotoscope have been fairly dark in nature. I had the training wheels of the models that came with the ML process that I adopted, but then I quickly began training on Salvador Dali and playing around with a distortion in time, but then found myself drowning in Nazi and Russian propaganda. I would take these models and apply them to normal everyday photos I have taken—obfuscating the darkness in the texture applied to each photo. Now, I am flipping the polarity, training more positive models, but then applying them to darker photos, experimenting with how I can further obfuscate the real world around was with digital textures. I have two models to demonstrate how I am seeing things.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://algorithmic.rotoscope.work/images/collections/bf-skinner-behaviorism/bf-skinner-old-piano-playing-hospital.jpg" /></entry><entry><title type="html">Rebuilding the Texture Transfer ML Process</title><link href="http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process/" rel="alternate" type="text/html" title="Rebuilding the Texture Transfer ML Process" /><published>2023-01-14T04:00:00-08:00</published><updated>2023-01-14T04:00:00-08:00</updated><id>http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process</id><content type="html" xml:base="http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process/">&lt;p&gt;I had an AWS machine image that had my texture transfer process all setup. I had my AWS account compromised and someone spun up servers across almost every AWS region—my bill was pushing $25K. Luckily it was due to anything I had done and AWS didn’t charge me for any of it. However, in the shuffle from that account to a new API Evangelist specific account, my AWS machine image got lost—forcing me to have to rebuild from scratch. It is always a daunting thing to dive back into the world of TensorFlow and machine learning, but I finally made time over this last holidays season. 
&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-bus-station.jpg&quot; alt=&quot;Bus Station&quot; /&gt;
I am using &lt;a href=&quot;https://github.com/lengstrom/fast-style-transfer&quot;&gt;lengstrom/fast-style-transfer&lt;/a&gt; for my texture transfer process. It really isn’t that hard to implement, but every time there is some learning curve around getting the server setup, all the TensorFlow libraries properly setup, and he keeps updating his process. This round I saw he was using Jupiter Notebooks, which I am not that interested in using, so I beat my own path forward. I am glad I did it one more time because there are a number of libraries I had to find from Archive.org to get to work, but finally I ended up with a working implementation. This time I have backed up on S3, and a local location. Ensuring that even if I lose my AMI I won’t lose the whole approach, cause I don’t think I will be able to rebuild again. The web is fragile that way. Not all of this is due to technical fragility, it is due to business fragility, disposability, and velocity. 
&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-freeway-camp.jpg&quot; alt=&quot;Bus Station&quot; /&gt;
Like many things, this approach to doing texture transfer will fall victim to what is next. You already see this happening with MidJourney and Dall-E approaches to images ML. It is compelling, exciting, and superior to what came before. I am just going to fork what is and keep applying in my own way off over here in this cul-de-sac. I’ll pay attention to what is new, cause I can’t help it, but I will also be experimenting with what was. For me, the layer of expression uses this particular ML model, but also a mix of textures and photos to find the sweet spot. Honestly, this sweet spot varies depending on what my mood is, how I am feeling about the world, and whether I have the money to create a new model or experiment with  the ones I already have.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">I had an AWS machine image that had my texture transfer process all setup. I had my AWS account compromised and someone spun up servers across almost every AWS region—my bill was pushing $25K. Luckily it was due to anything I had done and AWS didn’t charge me for any of it. However, in the shuffle from that account to a new API Evangelist specific account, my AWS machine image got lost—forcing me to have to rebuild from scratch. It is always a daunting thing to dive back into the world of TensorFlow and machine learning, but I finally made time over this last holidays season. I am using lengstrom/fast-style-transfer for my texture transfer process. It really isn’t that hard to implement, but every time there is some learning curve around getting the server setup, all the TensorFlow libraries properly setup, and he keeps updating his process. This round I saw he was using Jupiter Notebooks, which I am not that interested in using, so I beat my own path forward. I am glad I did it one more time because there are a number of libraries I had to find from Archive.org to get to work, but finally I ended up with a working implementation. This time I have backed up on S3, and a local location. Ensuring that even if I lose my AMI I won’t lose the whole approach, cause I don’t think I will be able to rebuild again. The web is fragile that way. Not all of this is due to technical fragility, it is due to business fragility, disposability, and velocity. Like many things, this approach to doing texture transfer will fall victim to what is next. You already see this happening with MidJourney and Dall-E approaches to images ML. It is compelling, exciting, and superior to what came before. I am just going to fork what is and keep applying in my own way off over here in this cul-de-sac. I’ll pay attention to what is new, cause I can’t help it, but I will also be experimenting with what was. For me, the layer of expression uses this particular ML model, but also a mix of textures and photos to find the sweet spot. Honestly, this sweet spot varies depending on what my mood is, how I am feeling about the world, and whether I have the money to create a new model or experiment with the ones I already have.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california.jpeg" /></entry><entry><title type="html">Landing on a Stable Instance of Tensorflow on Amazon to Train Models</title><link href="http://localhost:4000/2020/05/16/landiing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/" rel="alternate" type="text/html" title="Landing on a Stable Instance of Tensorflow on Amazon to Train Models" /><published>2020-05-16T05:00:00-07:00</published><updated>2020-05-16T05:00:00-07:00</updated><id>http://localhost:4000/2020/05/16/landiing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models</id><content type="html" xml:base="http://localhost:4000/2020/05/16/landiing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/">&lt;p&gt;I have bounced around quite a bit from the original Tensorflow model I was using when I started this work. Algorithmia had done a lot of the heavy lifting for me, but that original Amazon Web Services machine image eventually fell into disrepair and I couldn’t operate it anymore. For the last couple of years I bounced around trying different text transfer services, and kicking the tires on a variety of Tensorflow models that were cheaper to operate, but none of them produced the same results as some of the earlier models I had developed. To make things worse, I had accidentally deleted some of my earlier models, leaving me pretty determined to produce my own machine learning model.&lt;/p&gt;

&lt;p&gt;Well, after much experimentation I finally created one that works as well as the first model I had. Actually, it is the same model, I just managed to find the original source of how to build it, followed their instructions and then I was able to get up and going. I will do a formal write-up on it citing my sources, and walking through how I did what I did. I just haven’t had time to write it up – each time I go to operate it and apply to my current batch of images I have to retrain myself on how it all works. Eventually I’ve carve out enough time to write up an on-boarding document for it, and publish it here on the blog for this work.&lt;/p&gt;

&lt;p&gt;I’m just looking to draw a line in the sand here on the updates for when I finally got full control over how I train my models. So when I’m looking back I can better understand what images were from the first batch of models, what came the years in between, and what has been produced as part of this new process. There are distinct differences between each of the phases, how I trained each model, the length of time, and ultimately how I chose to apply them. Back in 2016 I had the money to run a GPU server for two months straight. I don’t have those kind of resources today, but I’m regularly funneling money into it whenever I can. Then step back occasionally and organized what I’ve produced into collections that I can use to showcase what I am seeing in my head when it comes to how AI, ML, APIs, and the Internet are being used to distort not just our online worlds, but our physical worlds as well.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Collections" /><category term="Models" /><summary type="html">I have bounced around quite a bit from the original Tensorflow model I was using when I started this work. Algorithmia had done a lot of the heavy lifting for me, but that original Amazon Web Services machine image eventually fell into disrepair and I couldn’t operate it anymore. For the last couple of years I bounced around trying different text transfer services, and kicking the tires on a variety of Tensorflow models that were cheaper to operate, but none of them produced the same results as some of the earlier models I had developed. To make things worse, I had accidentally deleted some of my earlier models, leaving me pretty determined to produce my own machine learning model.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /></entry><entry><title type="html">Adding Some Collections of My Algorotoscope Work</title><link href="http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work/" rel="alternate" type="text/html" title="Adding Some Collections of My Algorotoscope Work" /><published>2017-07-15T05:00:00-07:00</published><updated>2017-07-15T05:00:00-07:00</updated><id>http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work</id><content type="html" xml:base="http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work/">&lt;p&gt;After organizing a couple of the latest batches of images I produced as part of my algorotoscope work I wanted to step back and see what I had made. I wanted to get better at organizing some of the more interesting images I had produced using specific Tensorflow machine learning models. Much of my work is pretty random without much connection between the image I trained my models on and the images I applied my models to–they are just making for some interesting colors and textures. However, along the way I have been trying to squeeze more meaning as well as texture and colors out of the models I am training as well as the photos I take, and apply these models to.&lt;/p&gt;

&lt;p&gt;After reviewing my work I came across five separate filters I feel like are beginning to get at what I am seeing when it comes to algorithmic distortion on the web. Providing a collection of images that I feel represent this work well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/the-persistence-of-memory/&quot;&gt;&lt;strong&gt;The Persistence of Memory&lt;/strong&gt;&lt;/a&gt; - One of the first ML models I trained using Dali’s thought provoking work.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/copper-circuit/&quot;&gt;&lt;strong&gt;Copper Circuit&lt;/strong&gt;&lt;/a&gt; - One of my earlier models that I trained by leaving the world of art and using objects.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/nazi-invasion/&quot;&gt;&lt;strong&gt;Nazi Invasion&lt;/strong&gt;&lt;/a&gt; - Produced during a darker period after the Charlottesville rally rattled our world.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/nazi-poster/&quot;&gt;&lt;strong&gt;Nazi Poster&lt;/strong&gt;&lt;/a&gt; - A continuation of that work showing how the web is being used to promote hate and fear.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/russian-propaganda/&quot;&gt;&lt;strong&gt;Russian Propaganda&lt;/strong&gt;&lt;/a&gt; - Looking at how Russian propaganda is disseminated online, all the way up to the White House.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have several other collections simmering behind the scenes. I’ve bee producing some interesting photos and continue to generate ML models from interesting subjects.  I’ll add photos to these collections as new ones emerge, and I’ll add other collections when I get enough photos to round off each collection, and enough words to tell the story. It just makes me happy to surface some of these older images I had created, but were beginning to get buried by each newer wave of work.&lt;/p&gt;

&lt;p&gt;I am not happy with the majority of what I am producing, however I do have full control over how I train my models, as well as apply them to images–I haven’t for a couple of years now. This will help stabilize my work, helping me mine specific topics, time periods, and imagery. Currently I am looking for ways that I can help visualize how technology is being used to distort and provide disinformation when it comes to the COVID-19 pandemic. It will take me a while to focus in on some meaningful images that I can use, as well as take some new photos of common aspects of the world we live in right now.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Kin Lane" /><category term="Algorotoscope" /><summary type="html">After organizing a couple of the latest batches of images I produced as part of my algorotoscope work I wanted to step back and see what I had made. I wanted to get better at organizing some of the more interesting images I had produced using specific Tensorflow machine learning models. Much of my work is pretty random without much connection between the image I trained my models on and the images I applied my models to–they are just making for some interesting colors and textures. However, along the way I have been trying to squeeze more meaning as well as texture and colors out of the models I am training as well as the photos I take, and apply these models to.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /></entry><entry><title type="html">Showing What Algorithmic Influence On Markets Leaves Out</title><link href="http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/" rel="alternate" type="text/html" title="Showing What Algorithmic Influence On Markets Leaves Out" /><published>2017-07-15T05:00:00-07:00</published><updated>2017-07-15T05:00:00-07:00</updated><id>http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out</id><content type="html" xml:base="http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/">&lt;p&gt;I’ve been playing with different ways of visualizing the impact that algorithms are making on our lives. How they are being &lt;a href=&quot;http://kinlane.com/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;used to distort the immigration debate&lt;/a&gt;, and &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;how the current administration is being influenced and p0wned by Russian propaganda&lt;/a&gt;. I find shedding light on how algorithms are directly influencing a variety of conversations using machine learning a fun pastime. I’m also interested in finding ways to shine a light on what gets filtered out, omitted, censored, or completely forgotten by algorithms, and their authors.&lt;/p&gt;

&lt;p&gt;One of my latest filters I’ve trained using TensorFlow is called “Feed the People”. It is an early 20th century Soviet propaganda poster that I do not know much history behind, but I feel provides a compelling point, while also providing an attractive and usable color palette and textures–I will have to do more research on the back story. I took this propaganda poster and trained a TensorFlow machine learning model for about 24 hours on an AWS EC2 GPU instance, which cost me about $18.00 for the entire process–leaving me with a ML model I can apply to any image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg&quot; align=&quot;right&quot; width=&quot;98%&quot; style=&quot;padding: 15px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once I had my trained machine learning model I applied to a handful of images, including one I took of the economist Adam Smith statue in Edinburgh, Scotland–which interestingly was commissioned by &lt;a href=&quot;https://en.wikipedia.org/wiki/Adam_Smith_Institute&quot;&gt;the Adam Smith Institute (ASI)&lt;/a&gt;, a neoliberal (formerly libertarian) think tank and lobbying group based in the United Kingdom, named after Adam Smith, a Scottish moral philosopher and classical economist in 2003. Taking the essence of the “feed the people” propaganda and algorithmically transferring it an image of the famous economist from the 18th century that was installed on the city streets by a neoliberal think tank in 2003.&lt;/p&gt;

&lt;p&gt;I’m super fascinated by how algorithms influence markets, from high speed trading, all the way to how stories about markets are spread on Facebook by investors, and libertarian and neoliberal influencers. Algorithms are being used to distort, contort, p0wn, influence and create new markets. I am continuing to trying to understand how propaganda and ideology is influencing these algorithms, but more importantly highlighting the conversations, and people that are ultimately left behind in the cracks as algorithms continue to consume our digital and physical worlds, and disrupt everything along the way.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Kin Lane" /><category term="Algorotoscope" /><summary type="html">I’ve been playing with different ways of visualizing the impact that algorithms are making on our lives. How they are being used to distort the immigration debate, and how the current administration is being influenced and p0wned by Russian propaganda. I find shedding light on how algorithms are directly influencing a variety of conversations using machine learning a fun pastime. I’m also interested in finding ways to shine a light on what gets filtered out, omitted, censored, or completely forgotten by algorithms, and their authors.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /></entry><entry><title type="html">Highlighting Algorithmic Transparency Using My Algorotoscope Work</title><link href="http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/" rel="alternate" type="text/html" title="Highlighting Algorithmic Transparency Using My Algorotoscope Work" /><published>2017-06-28T05:00:00-07:00</published><updated>2017-06-28T05:00:00-07:00</updated><id>http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.%20markdown</id><content type="html" xml:base="http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/">&lt;p&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/updates/&quot; title=&quot;Algorithmic Rotoscope&quot;&gt;I started doing my algorotoscope work to better understand machine learning&lt;/a&gt;. I needed a hands-on project that would allow me to play with two overlapping slices of the machine learning pie–working with images and video. I wanted to understand what people meant when they said texture transfer or object recognition, and quantify the differences between machine learning providers, pulling the curtain back a little on a portion of machine learning, helping establish some transparency and observability.&lt;/p&gt;

&lt;p&gt;Algorotoscope allows me shine a light on machine learning, while also shining a light on the world of algorithms. I’m still learning what is possible, but the motivations behind &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;my Ellis Island Nazi Poster reflection&lt;/a&gt;, and &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;my White House Russian propaganda leaflet snapshot&lt;/a&gt; are meant to help me understand machine learning texture transfer models, and apply them to images in a way that helps demonstrate how algorithms are obfuscating the physical and digital world around us. Showcasing that algorithms are being used to distort the immigration debate, our elections, and almost every other aspect of our professional and personal lives.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/crypto-machine-bletchley_copper_circuit.png&quot; width=&quot;90%&quot; style=&quot;padding: 25px;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I understand technology by using it. Black box algorithms seem to be indistinguishable from magic for many folks, while they scare the hell out of me. Not because they contain magic, but because they contain exploitation, bias, corruption, privacy, and security nightmares. It is important to me that we understand the levers, knobs, dials, and gears behind algorithms. I am looking to use my algorotoscope work help reduce the distortion field that often surrounds algorithms, and how their various incarnations are being marketed. I want my readers to understand that nothing they read, no image they see, or video they watch is free of algorithmic influence, and that algorithms are making the decision about what you see, as well as what we do not see.&lt;/p&gt;

&lt;p&gt;Algorotscope is all about using machine learning to help us visualize the impact that algorithms are making on our world. I have no idea where the work is headed, except that I will keep working to generate relevant machine learning models trained on relevant images, then experiment with the application of these models as filters on images and video in a way that tells a story about how algorithms are distorting our world, and shifting how we view things both on and offline. I’m looking to move my &lt;a href=&quot;http://apievangelist.com&quot;&gt;API Evangelist storytelling to use 100% algorotscope images&lt;/a&gt;, as I keep scratching the surface of how algorithms are invading our lives via the web, devices, and everyday objects.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Algorotoscope" /><category term="Algorithms" /><summary type="html">I started doing my algorotoscope work to better understand machine learning. I needed a hands-on project that would allow me to play with two overlapping slices of the machine learning pie–working with images and video. I wanted to understand what people meant when they said texture transfer or object recognition, and quantify the differences between machine learning providers, pulling the curtain back a little on a portion of machine learning, helping establish some transparency and observability.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://algorithmic.rotoscope.work/updates/ &quot;Algorithmic Rotoscope" /></entry><entry><title type="html">Why Would People Want Fine Art Trained Machine Learning Models</title><link href="http://localhost:4000/2017/03/08/why-would-people-want-fine-art-trained-machine-learning-models/" rel="alternate" type="text/html" title="Why Would People Want Fine Art Trained Machine Learning Models" /><published>2017-03-08T04:00:00-08:00</published><updated>2017-03-08T04:00:00-08:00</updated><id>http://localhost:4000/2017/03/08/why-would-people-want-fine-art-trained-machine-learning-models</id><content type="html" xml:base="http://localhost:4000/2017/03/08/why-would-people-want-fine-art-trained-machine-learning-models/">&lt;p&gt;I'm spending time on my &lt;a href=&quot;http://algorithmic.rotoscope.work/&quot;&gt;algorithmic rotoscope work&lt;/a&gt;, and thinking about how the machine learning style textures I've been marking can be put to use. I'm trying to see things from different vantage points and develop a better understanding of how texture styles can be put to use in the regular world.&lt;/p&gt;
&lt;p&gt;I am enjoying using image style filters in my writing. It gives me kind of a gamified layer to my photography and drone hobby that allows me to create actual images I can use in my work as the &lt;a href=&quot;http://apievangelist.com&quot;&gt;API Evangelist&lt;/a&gt;. Having unique filtered images available for use in my writing is valuable to me--enough to justify the couple hundreds of dollars I spend each month on AWS servers.&lt;/p&gt;
&lt;p&gt;I know why I like applying image styles to my photos, but why do others? Most of the image filters out there we've seen from apps like &lt;a href=&quot;https://prisma-ai.com/&quot;&gt;Prisma&lt;/a&gt; are focused on fine art. Training image style transfer machine learning models on popular art that people are already familiar with. I guess this is allows people to apply the characteristics of art they like to the photographic layer of our increasingly digital lives.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_brush.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;To me, it feels like some sort of art placebo. A way of superficially and algorithmic injecting what are brain tells us is artsy to our fairly empty, digital photo reality. Taking photos in real time isn't satisfying enough anymore. We need to distract ourselves from the world by applying reality to our digitally documented physical world--almost the opposite of augmented reality if there is such a thing. Getting lost in the ability to look at the real world through the algorithmic lens of our online life.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_alien_goggles.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We are stealing the essence the meaningful, tangible art from our real world, and digitizing it. We take this essense and algorithmically apply it our everyday life trying to add some color, some texture, but not too much. We need the photos to still be meaningful, and have context in our life, but we need to be able to spray an algorithmic lacquer of meaning on our intangible lives.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_crunch_paper.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The more filters we have, the more lenses we have to look at the exact same moment we live each day. We go to work. We go to school. We see the same scenery, the same people, and the same pictures each day. Now we are able to algorithmic shift, distort, and paint the picture of our lives we want to see.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_dark_dali.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can add color to our life. We are being trained to think we can change the palette, and are in control over our lives. We can colorize the old World War 2 era photos of our day, and choose whether we want to color within, or outside the lines. Our lives don't have to be just binary 1s and 0s, and black or white.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_kandinsky_bv.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Slowly, picture by picture, algorithmic transfer by algorithmic transfer, the way we see the world changes. We no longer settle for the way things are, the way our mobile phone camera catches it. The digital version is the image we share with my friends, family, and the world. It should always be the most brilliant, the most colorful, and the painting that catches their eye and makes them stand in front of on the wall of your Facebook feed captivated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_sunday.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We no longer will remember what reality looks like, or what art looks like. Our collective social media memory will dictate what the world looks like. The number of likes will determine what is artistic, and what is beautiful or ugly. The algorithm will only show us what images match the world it wants us to see. Algorithmically, artistically painting the inside walls of our digital bubble.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Eventually, the sensors that stimulate us when we see photos will be well worn. They will be well programmed, with known inputs, and predictable outputs. The algorithm will be able to deliver exactly what we need, and correctly predict what we will need next. Scheduling up and queuing the next fifty possible scenarios--with exactly the right colors, textures, and meaning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_copper_circuit_2.jpg&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;How we see art will be forever changed by the algorithm. Our machines will never see art. Our machines will never know art. Our machines will only be able to transfer the characteristics we see and deliver them into newer, more relevant, timely, and meaningful images. Distilling down the essence of art into binary, and programming us to think this synthetic art is meaningful, and still applies to our physical world.&lt;/p&gt;
&lt;p&gt;Like I said, I think people like applying artistic image filters to their mobile photos because it is the opposite of augmented reality. They are trying to augment their digital (hopes of reality) presence with the essence of what we (algorithm) think matters to use in the world. This process isn't about training a model to see art like some folks may tell you. It is about distilling down some of the most simple aspects of what our eyes see as art, and give this algorithm to our mobile phones and social networks to apply to the photograph digital logging of our physical reality. &lt;/p&gt;
&lt;p&gt;It feels like this is about reprogramming people. It is about reprogramming what stimulates you. Automating an algorithmic view of what matters when it comes to art, and applying it to a digital view of matters in our daily worlds, via our social networks. Just one more area of our life where we are allowing algorithms to reprogram us, and bend our reality to be more digital.&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://umma.umaine.edu/&quot;&gt;I Borrowed This Image From University of Maine Museum of Art&lt;/a&gt;&lt;/p&gt;</content><author><name>Kin Lane</name></author><summary type="html">I'm spending time on my algorithmic rotoscope work, and thinking about how the machine learning style textures I've been marking can be put to use. I'm trying to see things from different vantage points and develop a better understanding of how texture styles can be put to use in the regular world. I am enjoying using image style filters in my writing. It gives me kind of a gamified layer to my photography and drone hobby that allows me to create actual images I can use in my work as the API Evangelist. Having unique filtered images available for use in my writing is valuable to me--enough to justify the couple hundreds of dollars I spend each month on AWS servers. I know why I like applying image styles to my photos, but why do others? Most of the image filters out there we've seen from apps like Prisma are focused on fine art. Training image style transfer machine learning models on popular art that people are already familiar with. I guess this is allows people to apply the characteristics of art they like to the photographic layer of our increasingly digital lives. To me, it feels like some sort of art placebo. A way of superficially and algorithmic injecting what are brain tells us is artsy to our fairly empty, digital photo reality. Taking photos in real time isn't satisfying enough anymore. We need to distract ourselves from the world by applying reality to our digitally documented physical world--almost the opposite of augmented reality if there is such a thing. Getting lost in the ability to look at the real world through the algorithmic lens of our online life. We are stealing the essence the meaningful, tangible art from our real world, and digitizing it. We take this essense and algorithmically apply it our everyday life trying to add some color, some texture, but not too much. We need the photos to still be meaningful, and have context in our life, but we need to be able to spray an algorithmic lacquer of meaning on our intangible lives. The more filters we have, the more lenses we have to look at the exact same moment we live each day. We go to work. We go to school. We see the same scenery, the same people, and the same pictures each day. Now we are able to algorithmic shift, distort, and paint the picture of our lives we want to see. Now we can add color to our life. We are being trained to think we can change the palette, and are in control over our lives. We can colorize the old World War 2 era photos of our day, and choose whether we want to color within, or outside the lines. Our lives don't have to be just binary 1s and 0s, and black or white. Slowly, picture by picture, algorithmic transfer by algorithmic transfer, the way we see the world changes. We no longer settle for the way things are, the way our mobile phone camera catches it. The digital version is the image we share with my friends, family, and the world. It should always be the most brilliant, the most colorful, and the painting that catches their eye and makes them stand in front of on the wall of your Facebook feed captivated. We no longer will remember what reality looks like, or what art looks like. Our collective social media memory will dictate what the world looks like. The number of likes will determine what is artistic, and what is beautiful or ugly. The algorithm will only show us what images match the world it wants us to see. Algorithmically, artistically painting the inside walls of our digital bubble. Eventually, the sensors that stimulate us when we see photos will be well worn. They will be well programmed, with known inputs, and predictable outputs. The algorithm will be able to deliver exactly what we need, and correctly predict what we will need next. Scheduling up and queuing the next fifty possible scenarios--with exactly the right colors, textures, and meaning. How we see art will be forever changed by the algorithm. Our machines will never see art. Our machines will never know art. Our machines will only be able to transfer the characteristics we see and deliver them into newer, more relevant, timely, and meaningful images. Distilling down the essence of art into binary, and programming us to think this synthetic art is meaningful, and still applies to our physical world. Like I said, I think people like applying artistic image filters to their mobile photos because it is the opposite of augmented reality. They are trying to augment their digital (hopes of reality) presence with the essence of what we (algorithm) think matters to use in the world. This process isn't about training a model to see art like some folks may tell you. It is about distilling down some of the most simple aspects of what our eyes see as art, and give this algorithm to our mobile phones and social networks to apply to the photograph digital logging of our physical reality. It feels like this is about reprogramming people. It is about reprogramming what stimulates you. Automating an algorithmic view of what matters when it comes to art, and applying it to a digital view of matters in our daily worlds, via our social networks. Just one more area of our life where we are allowing algorithms to reprogram us, and bend our reality to be more digital. I Borrowed This Image From University of Maine Museum of Art</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_brush.jpg" /></entry><entry><title type="html">Machine Learning Style Transfer For Museums, Libraries, and Collections</title><link href="http://localhost:4000/2017/03/07/machine-learning-style-transfer-for-museums-libraries-and-collections/" rel="alternate" type="text/html" title="Machine Learning Style Transfer For Museums, Libraries, and Collections" /><published>2017-03-07T11:00:00-08:00</published><updated>2017-03-07T11:00:00-08:00</updated><id>http://localhost:4000/2017/03/07/machine-learning-style-transfer-for-museums-libraries-and-collections</id><content type="html" xml:base="http://localhost:4000/2017/03/07/machine-learning-style-transfer-for-museums-libraries-and-collections/">&lt;p&gt;&lt;img style=&quot;padding: 15px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/art_museum_kandinsky_bv.jpg&quot; alt=&quot;&quot; width=&quot;40%&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I putting some thought into some next steps for my &lt;a href=&quot;http://algorithmic.rotoscope.work/&quot;&gt;algorithmic rotoscope work&lt;/a&gt;, which is about the training and applying of image style transfer machine learning models. I'm talking with Jason Toy (&lt;a href=&quot;https://twitter.com/jtoy&quot;&gt;@jtoy&lt;/a&gt;) over at&amp;nbsp;&lt;a href=&quot;http://www.somatic.io/&quot;&gt;Somatic&lt;/a&gt; about the variety of use cases, and I want to spend some thinking about image style transfers, from the perspective of a collector or curator of images--brainstorming how they can organize, make available their work(s) for use in image style transfers.&lt;/p&gt;
&lt;p&gt;Ok, let's start with the basics--what am I talking about when I say image style transfer? &amp;nbsp;&lt;a href=&quot;http://kinlane.com/2017/02/15/what-do-you-mean-when-you-say-you-are-training-a-machine-learning-model/&quot;&gt;I recommend starting with a basic definition of machine learning in this context, providing by my girlfriend, and partner in crime Audrey Watters&lt;/a&gt;. Beyond, that I am just referring to the training a machine learning model by directing it to scan an image. This model can then be applied to other images, essentially transferring the style of one image, to any other image. There are a handful of mobile applications out there right now that let you apply a handful of filters to images taken with your mobile phone--&lt;a href=&quot;http://www.somatic.io/&quot;&gt;Somatic is looking to be the wholesale provider of these features&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Training one of these models isn't cheap. It costs me about $20 per model in GPUs to create--this doesn't consider my time, just my hard compute costs (AWS bill). Not every model does anything interesting. Not all images, photos, and pieces of art translate&amp;nbsp;into cool features when applied to images. I've spent about $700 training 35 filters. Some of them are cool, and some of them are meh. I've had the most luck focusing on &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/01/03/finding-the-right-dystopian-filter-to-represent-the-world-unfolding-around-us/&quot;&gt;dystopian landscapes&lt;/a&gt;, which I can use in my storytelling around topics like &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;immigration&lt;/a&gt;, technology, and the &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;election&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This work ended up with Jason and I talking about museums and library collections, thinking about opportunities for them to think about their collections in terms of machine learning, and specifically algorithmic style transfer. Do you have images in your collection that would translate well for use in graphic design, print, and digital photo applications? I spend hours looking through art books for the right textures, colors and outlines. I also spend hours looking through graphic design archives for movie and gaming industry, as well as government collections. Looking for just the right set of images that will either transfer&amp;nbsp;and produce an interesting look, as well as possible transfer something meaningful to the new images that I am applying styles to.&lt;/p&gt;
&lt;p&gt;Sometimes style transfers just make a photo look cool, bringing some general colors, textures, and other features to a new photo--there really isn't any value in knowing what image was behind the style transfer, it just looks cool. Other times, the image can be enhanced knowing about the image behind the machine learning model, and not just transferring&amp;nbsp;styles between images, but also potentially transferring some meaning as well. You can see this in action when I took &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;a nazi propaganda poster and applied to it to photo of Ellis Island&lt;/a&gt;, or I took an &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;old Russian propaganda poster and applied to images of the White House&lt;/a&gt;. I a sense, I was able to transfer some of the 1000 words applied to the propaganda posters and transfer them to new photos I had taken.&lt;/p&gt;
&lt;p&gt;It's easy to think you will make a new image into a piece of art by training a model on a piece of art and transferring it's characteristics to a new image using machine learning. Where I find the real value is actually understanding collections of images, while also being aware of the style transfer process, and thinking about how images can be trained and applied. However, this only gets you so far, there has to still be some value or meaning in how it's being applied, accomplishing a specific objective and delivering some sort of meaning. If you are doing this as part of some graphic design work it will be different than if you are doing for fun on a mobile phone app with your friends.&lt;/p&gt;
&lt;p&gt;To further stimulate my imagination and awareness I'm looking through a variety of open image collections, from a variety of institutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://dp.la/&quot;&gt;Digital Public Library of America (DPLA)&lt;/a&gt;&lt;/strong&gt; -&amp;nbsp;The DPLA is a platform. Developers make apps that use the library&amp;rsquo;s data in many different ways.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.flickr.com/photos/britishlibrary/&quot;&gt;The British Library&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;A collection of over 1 million public domain images from digitized copies of 17th, 18th, and 19th-century books.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://www.europeana.eu/portal/&quot;&gt;Europeana&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Explore millions of items from a range of Europe's leading galleries, libraries, archives and museums.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://www.loc.gov/rr/print/&quot;&gt;The Library of Congress Prints &amp;amp; Photographs Reading Room&lt;/a&gt;&amp;nbsp;- &lt;/strong&gt;Photographs, historical prints, posters, cartoons, documentary drawings, fine prints, and architectural and engineering designs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://www.metmuseum.org/research/image-resources#scholarly&quot;&gt;Metropolitan Museum of Art&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Selected artworks are available under the Open Access for Scholarly Content (OASC) license.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.nasa.gov/multimedia/imagegallery/index.html&quot;&gt;&lt;strong&gt;National Aeronautics and Space Administration (NASA) Image Galleries&lt;/strong&gt;&lt;/a&gt;: Public domain photos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://www.nypl.org/publicdomain&quot;&gt;The New York Public Library Digital Collections&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Digital collections of&amp;nbsp;high resolution&amp;nbsp;prints, images, and maps with no known copyright restrictions. Rights statement included for individual items.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://hsl.osu.edu/resources/subject-guides/guide/digital-image-collections&quot;&gt;The Ohio State University Health Sciences Library Digital Image Collections&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;A list of resources available through the OSU Health Science Library.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://phil.cdc.gov/phil/&quot;&gt;Public Health Image Library (PHIL)&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Provided by the Centers for Disease Control and Prevention.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://archive.samuelzeller.ch/&quot;&gt;Samuel Zeller Archive&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Professional photographs available under a Creative Commons Attribution (CC BY) license.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://search.usa.gov/search/images?affiliate=usagov&amp;amp;query=&quot;&gt;U.S. Government Photos and Images&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;A collection of photo and image galleries for multiple federal government agencies.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am also using some of the usual suspects when it comes to searching for images on the web:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.google.com/imghp&quot;&gt;Google Image Search&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;The old standby place to work through ideas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.flickr.com/creativecommons/&quot;&gt;Flickr: Creative Commons&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Many Flickr users have chosen to offer their work under a Creative Commons license, and you can browse or search through content under each type of license.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.flickr.com/commons&quot;&gt;Flickr: The Commons&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Includes photos with&amp;nbsp;&lt;a href=&quot;https://www.flickr.com/commons/usage/&quot;&gt;&amp;ldquo;no known copyright restrictions&amp;rdquo;&lt;/a&gt;&amp;nbsp;uploaded by participating cultural institutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://archive.org/index.php&quot;&gt;Internet Archive&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Contains books, movies, software, music, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://www.pond5.com/free&quot;&gt;pond5 Public Domain Project&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;Public domain images. Registration/free account required to download materials.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://commons.wikimedia.org/wiki/Main_Page&quot;&gt;Wikimedia Commons&lt;/a&gt;&amp;nbsp;-&amp;nbsp;&lt;/strong&gt;A database of&amp;nbsp;&lt;a href=&quot;https://commons.wikimedia.org/wiki/Commons:Reusing_content_outside_Wikimedia&quot;&gt;freely usable&lt;/a&gt;&amp;nbsp;media files to which&amp;nbsp;anyone can contribute.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img style=&quot;padding: 15px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/propaganda_poster_transfer_1.jpg&quot; alt=&quot;&quot; width=&quot;40%&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I am working on developing specific categories that have relevance to the storytelling I'm doing across my blogs, and sometimes to help power my&amp;nbsp;partners work as well. I'm currently mining the following areas, looking for interesting images to train style transfer machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Art&lt;/strong&gt; - The obvious usage for all of this, finding interesting pieces of art that make your photos look cool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video Game -&lt;/strong&gt; I find video game imagery to provide a wealth of ideas for training and applying image style transfers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Science Fiction &lt;/strong&gt;- Another rich source of imagery for the&amp;nbsp;training of image style transfer models that do cool things.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Electrical&lt;/strong&gt; - I'm finding circuit boards, lighting, and other electrical imagery to be useful in training models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industrial&lt;/strong&gt; - I'm finding industrial images to work for both sides of the equation in training and applying models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propaganda&lt;/strong&gt; - These are great for training models, and then transferring the texture and the meaning behind them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Labor&lt;/strong&gt; - Similar to propaganda posters, potentially some emotional work here that would transfer significant meaning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space&lt;/strong&gt; - A new one I'm adding for finding interesting imagery that can train models, and experiencing what the effect is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I look through more collections, and gain experience training style transfer models, and applying models, I have begun to develop an eye for what looks good. I also develop more ideas along the way of imagery that can help reinforce the storytelling I'm doing across my work. It is a journey I am hoping more librarians, museum curators, and collection stewards will embark on. I don't think you need to learn the inner workings of machine learning, but at least develop enough of an understanding that you can think more critically about the collection you are knowledgeable about.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img style=&quot;padding: 15px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/propaganda_poster_transfer_2.jpg&quot; alt=&quot;&quot; width=&quot;40%&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I know Jason would like to help you, and I'm more than happy to help you along in the process. Honestly, the biggest hurdle is money to afford the GPUs for training the image. After that, it is about spending the time finding images to train models, as well as to apply the models to a variety of imagery, as part of some sort of meaningful process. I can spend days looking through art collection, then spend a significant amount of AWS budget training machine learning models, but if I don't have a meaningful way to apply them, it doesn't bring any value to the table, and it's unlikely I will be able to justify the budget in the future.&lt;/p&gt;
&lt;p&gt;My algorithmic rotoscope work is used throughout my writing&amp;nbsp;and helps influence the stories I tell on &lt;a href=&quot;http://apievangelist.com&quot;&gt;API Evangelist&lt;/a&gt;, &lt;a href=&quot;http://kinlane.com&quot;&gt;Kin Lane&lt;/a&gt;, &lt;a href=&quot;http://dronerecovery.org&quot;&gt;Drone Recovery&lt;/a&gt;, and now &lt;a href=&quot;http://contrafabulists.com&quot;&gt;Contrafabulists&lt;/a&gt;. I invest about $150.00 / month training to image style transfer models, keeping a fresh number of models coming off the assembly line. I have a variety of tools that allow me to apply the models using &lt;a href=&quot;http://algorithmia.com&quot;&gt;Algorithmia&lt;/a&gt; and now &lt;a href=&quot;http://somatic.io&quot;&gt;Somatic&lt;/a&gt;. I'm now looking for folks who have knowledge and access to interesting image collections, who would want to learn more about image style transfer, as well as graphic design and print shops, mobile application development shops, and other interested folks who are just curious about WTF image style transfers are all about.&lt;/p&gt;</content><author><name>Kin Lane</name></author><summary type="html">I putting some thought into some next steps for my algorithmic rotoscope work, which is about the training and applying of image style transfer machine learning models. I'm talking with Jason Toy (@jtoy) over at&amp;nbsp;Somatic about the variety of use cases, and I want to spend some thinking about image style transfers, from the perspective of a collector or curator of images--brainstorming how they can organize, make available their work(s) for use in image style transfers. Ok, let's start with the basics--what am I talking about when I say image style transfer? &amp;nbsp;I recommend starting with a basic definition of machine learning in this context, providing by my girlfriend, and partner in crime Audrey Watters. Beyond, that I am just referring to the training a machine learning model by directing it to scan an image. This model can then be applied to other images, essentially transferring the style of one image, to any other image. There are a handful of mobile applications out there right now that let you apply a handful of filters to images taken with your mobile phone--Somatic is looking to be the wholesale provider of these features.&amp;nbsp; Training one of these models isn't cheap. It costs me about $20 per model in GPUs to create--this doesn't consider my time, just my hard compute costs (AWS bill). Not every model does anything interesting. Not all images, photos, and pieces of art translate&amp;nbsp;into cool features when applied to images. I've spent about $700 training 35 filters. Some of them are cool, and some of them are meh. I've had the most luck focusing on dystopian landscapes, which I can use in my storytelling around topics like immigration, technology, and the election.&amp;nbsp; This work ended up with Jason and I talking about museums and library collections, thinking about opportunities for them to think about their collections in terms of machine learning, and specifically algorithmic style transfer. Do you have images in your collection that would translate well for use in graphic design, print, and digital photo applications? I spend hours looking through art books for the right textures, colors and outlines. I also spend hours looking through graphic design archives for movie and gaming industry, as well as government collections. Looking for just the right set of images that will either transfer&amp;nbsp;and produce an interesting look, as well as possible transfer something meaningful to the new images that I am applying styles to. Sometimes style transfers just make a photo look cool, bringing some general colors, textures, and other features to a new photo--there really isn't any value in knowing what image was behind the style transfer, it just looks cool. Other times, the image can be enhanced knowing about the image behind the machine learning model, and not just transferring&amp;nbsp;styles between images, but also potentially transferring some meaning as well. You can see this in action when I took a nazi propaganda poster and applied to it to photo of Ellis Island, or I took an old Russian propaganda poster and applied to images of the White House. I a sense, I was able to transfer some of the 1000 words applied to the propaganda posters and transfer them to new photos I had taken. It's easy to think you will make a new image into a piece of art by training a model on a piece of art and transferring it's characteristics to a new image using machine learning. Where I find the real value is actually understanding collections of images, while also being aware of the style transfer process, and thinking about how images can be trained and applied. However, this only gets you so far, there has to still be some value or meaning in how it's being applied, accomplishing a specific objective and delivering some sort of meaning. If you are doing this as part of some graphic design work it will be different than if you are doing for fun on a mobile phone app with your friends. To further stimulate my imagination and awareness I'm looking through a variety of open image collections, from a variety of institutions: Digital Public Library of America (DPLA) -&amp;nbsp;The DPLA is a platform. Developers make apps that use the library&amp;rsquo;s data in many different ways. The British Library&amp;nbsp;-&amp;nbsp;A collection of over 1 million public domain images from digitized copies of 17th, 18th, and 19th-century books. Europeana&amp;nbsp;-&amp;nbsp;Explore millions of items from a range of Europe's leading galleries, libraries, archives and museums. The Library of Congress Prints &amp;amp; Photographs Reading Room&amp;nbsp;- Photographs, historical prints, posters, cartoons, documentary drawings, fine prints, and architectural and engineering designs. Metropolitan Museum of Art&amp;nbsp;-&amp;nbsp;Selected artworks are available under the Open Access for Scholarly Content (OASC) license. National Aeronautics and Space Administration (NASA) Image Galleries: Public domain photos. The New York Public Library Digital Collections&amp;nbsp;-&amp;nbsp;Digital collections of&amp;nbsp;high resolution&amp;nbsp;prints, images, and maps with no known copyright restrictions. Rights statement included for individual items. The Ohio State University Health Sciences Library Digital Image Collections&amp;nbsp;-&amp;nbsp;A list of resources available through the OSU Health Science Library. Public Health Image Library (PHIL)&amp;nbsp;-&amp;nbsp;Provided by the Centers for Disease Control and Prevention.&amp;nbsp; Samuel Zeller Archive&amp;nbsp;-&amp;nbsp;Professional photographs available under a Creative Commons Attribution (CC BY) license. U.S. Government Photos and Images&amp;nbsp;-&amp;nbsp;A collection of photo and image galleries for multiple federal government agencies.&amp;nbsp; I am also using some of the usual suspects when it comes to searching for images on the web: Google Image Search&amp;nbsp;-&amp;nbsp;The old standby place to work through ideas. Flickr: Creative Commons&amp;nbsp;-&amp;nbsp;Many Flickr users have chosen to offer their work under a Creative Commons license, and you can browse or search through content under each type of license. Flickr: The Commons&amp;nbsp;-&amp;nbsp;Includes photos with&amp;nbsp;&amp;ldquo;no known copyright restrictions&amp;rdquo;&amp;nbsp;uploaded by participating cultural institutions. Internet Archive&amp;nbsp;-&amp;nbsp;Contains books, movies, software, music, and more. pond5 Public Domain Project&amp;nbsp;-&amp;nbsp;Public domain images. Registration/free account required to download materials.&amp;nbsp; Wikimedia Commons&amp;nbsp;-&amp;nbsp;A database of&amp;nbsp;freely usable&amp;nbsp;media files to which&amp;nbsp;anyone can contribute. I am working on developing specific categories that have relevance to the storytelling I'm doing across my blogs, and sometimes to help power my&amp;nbsp;partners work as well. I'm currently mining the following areas, looking for interesting images to train style transfer machine learning models: Art - The obvious usage for all of this, finding interesting pieces of art that make your photos look cool. Video Game - I find video game imagery to provide a wealth of ideas for training and applying image style transfers. Science Fiction - Another rich source of imagery for the&amp;nbsp;training of image style transfer models that do cool things. Electrical - I'm finding circuit boards, lighting, and other electrical imagery to be useful in training models. Industrial - I'm finding industrial images to work for both sides of the equation in training and applying models. Propaganda - These are great for training models, and then transferring the texture and the meaning behind them. Labor - Similar to propaganda posters, potentially some emotional work here that would transfer significant meaning. Space - A new one I'm adding for finding interesting imagery that can train models, and experiencing what the effect is. As I look through more collections, and gain experience training style transfer models, and applying models, I have begun to develop an eye for what looks good. I also develop more ideas along the way of imagery that can help reinforce the storytelling I'm doing across my work. It is a journey I am hoping more librarians, museum curators, and collection stewards will embark on. I don't think you need to learn the inner workings of machine learning, but at least develop enough of an understanding that you can think more critically about the collection you are knowledgeable about.&amp;nbsp; I know Jason would like to help you, and I'm more than happy to help you along in the process. Honestly, the biggest hurdle is money to afford the GPUs for training the image. After that, it is about spending the time finding images to train models, as well as to apply the models to a variety of imagery, as part of some sort of meaningful process. I can spend days looking through art collection, then spend a significant amount of AWS budget training machine learning models, but if I don't have a meaningful way to apply them, it doesn't bring any value to the table, and it's unlikely I will be able to justify the budget in the future. My algorithmic rotoscope work is used throughout my writing&amp;nbsp;and helps influence the stories I tell on API Evangelist, Kin Lane, Drone Recovery, and now Contrafabulists. I invest about $150.00 / month training to image style transfer models, keeping a fresh number of models coming off the assembly line. I have a variety of tools that allow me to apply the models using Algorithmia and now Somatic. I'm now looking for folks who have knowledge and access to interesting image collections, who would want to learn more about image style transfer, as well as graphic design and print shops, mobile application development shops, and other interested folks who are just curious about WTF image style transfers are all about.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/propaganda_poster_transfer_2.jpg" /></entry><entry><title type="html">The Russian Propaganda Distortion Field Around The White House</title><link href="http://localhost:4000/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/" rel="alternate" type="text/html" title="The Russian Propaganda Distortion Field Around The White House" /><published>2017-02-14T15:00:00-08:00</published><updated>2017-02-14T15:00:00-08:00</updated><id>http://localhost:4000/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house</id><content type="html" xml:base="http://localhost:4000/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/">&lt;p&gt;&lt;img style=&quot;padding: 15px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/propagandaleaflets.jpg&quot; alt=&quot;&quot; width=&quot;25%&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I am having a difficult time reconciling what is going on with the White House right now. The distortion field around the administration right now feels like some bad acid trip from the 1980s, before I learned how to find the good LSD. After losing their shit over her emails and Benghazi, they are willing to overlook Russia fucking with our election on so many levels and infiltrating the White House. Wacky. Just fucking wacky!&lt;/p&gt;
&lt;p&gt;The way Russia has fed the poor folk in this country a steady diet of bullshit is pretty crafty, as well as disturbing. Their approach to disinformation has dovetailed nicely with the approach of the GOP in this country. As usual, I am trying to understand and visualize the algorithmic distortion in this conversation, and how our current administration could be so heavily under the influence of Russian propaganda.&lt;/p&gt;
&lt;p&gt;I'm going through Russian propaganda archives looking for the right colors and textures to shine a light on the algorithmic distortion raining down on the White House as part of this ongoing&amp;nbsp;Russian cyber attack. I'm using the posters I've found to train some machine learning models, and the first one has come off the cloud pipeline and was ready for applying to some images to see what the effect might be. I started with a couple photos I've taken of the White House. One from the lawn, and one from inside the Eisenhower Executive Office Building (EEOB) while I was working there.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;I like the results. It makes it look like the distortion field around the White House are just dense pamphlets raining down from above, dens like Internet packets aggregating&amp;nbsp;on a wireless&amp;nbsp;network fighting to get in. It's fascinating to watch people be so willfully ignorant to see the algorithmic distortion around them. Even with all the talk of wireless, mobile, the web, and cyber warfare. They don't see how they are under assault from information and disinformation--something the Russians seem to excel at.&lt;/p&gt;
&lt;p&gt;There are 3 other posters being used to train machine learning models right now. I can only do one at a time and each one takes about 12 hours. Then it will take me about another week or so of applying them to images to find what works and doesn't work with the filters. I have about 40 individual filters currently, and I have been focusing heavily on dystopian textures in the previous round. I am thinking that this round I'll focus on colors and textures I can use to highlight the effects of the cyber on our reality -- I hear it is going to be huge.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Here is the look from next door....&lt;/p&gt;
&lt;p&gt;&lt;img style=&quot;padding: 3px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg&quot; alt=&quot;&quot; width=&quot;99%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the view from the front lawn...&lt;/p&gt;
&lt;p&gt;&lt;img style=&quot;padding: 3px;&quot; src=&quot;http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_lawn_propaganda_leaflets.jpg&quot; alt=&quot;&quot; width=&quot;99%&quot; /&gt;&lt;/p&gt;</content><author><name>Kin Lane</name></author><summary type="html">I am having a difficult time reconciling what is going on with the White House right now. The distortion field around the administration right now feels like some bad acid trip from the 1980s, before I learned how to find the good LSD. After losing their shit over her emails and Benghazi, they are willing to overlook Russia fucking with our election on so many levels and infiltrating the White House. Wacky. Just fucking wacky! The way Russia has fed the poor folk in this country a steady diet of bullshit is pretty crafty, as well as disturbing. Their approach to disinformation has dovetailed nicely with the approach of the GOP in this country. As usual, I am trying to understand and visualize the algorithmic distortion in this conversation, and how our current administration could be so heavily under the influence of Russian propaganda. I'm going through Russian propaganda archives looking for the right colors and textures to shine a light on the algorithmic distortion raining down on the White House as part of this ongoing&amp;nbsp;Russian cyber attack. I'm using the posters I've found to train some machine learning models, and the first one has come off the cloud pipeline and was ready for applying to some images to see what the effect might be. I started with a couple photos I've taken of the White House. One from the lawn, and one from inside the Eisenhower Executive Office Building (EEOB) while I was working there.&amp;nbsp; I like the results. It makes it look like the distortion field around the White House are just dense pamphlets raining down from above, dens like Internet packets aggregating&amp;nbsp;on a wireless&amp;nbsp;network fighting to get in. It's fascinating to watch people be so willfully ignorant to see the algorithmic distortion around them. Even with all the talk of wireless, mobile, the web, and cyber warfare. They don't see how they are under assault from information and disinformation--something the Russians seem to excel at. There are 3 other posters being used to train machine learning models right now. I can only do one at a time and each one takes about 12 hours. Then it will take me about another week or so of applying them to images to find what works and doesn't work with the filters. I have about 40 individual filters currently, and I have been focusing heavily on dystopian textures in the previous round. I am thinking that this round I'll focus on colors and textures I can use to highlight the effects of the cyber on our reality -- I hear it is going to be huge.&amp;nbsp; Here is the look from next door.... Here is the view from the front lawn...</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/white_house_lawn_propaganda_leaflets.jpg" /></entry></feed>