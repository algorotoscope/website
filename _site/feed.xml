<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-11-10T18:57:45-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kin Lane</title><subtitle>The ramblings, thoughts, and channeling of Kin Lane.</subtitle><author><name>Kin Lane</name></author><entry><title type="html">Composite Algorotoscope Images</title><link href="http://localhost:4000/2024/03/23/composite-algorotoscope-images/" rel="alternate" type="text/html" title="Composite Algorotoscope Images" /><published>2024-03-23T08:00:00-04:00</published><updated>2024-03-23T08:00:00-04:00</updated><id>http://localhost:4000/2024/03/23/composite-algorotoscope-images</id><content type="html" xml:base="http://localhost:4000/2024/03/23/composite-algorotoscope-images/">&lt;p&gt;I spent some time making some new images with my B.F. Skinner Algorotoscope models the other weekend. I am pretty happy with my pallet of AI models right now. I need to get them better displays don the site, but the tones of them represent many of the emotions I am trying to illuminate right now. However, in th mean time I am now exploring how I can better overlay images produced from different Algorotoscope models. &lt;a href=&quot;https://algorithmic.rotoscope.work/collections/bf-skinner-behaviorism/&quot;&gt;The B.F. Skinner Algorotoscope model&lt;/a&gt; sets the stage for many technological stories I am wanting to tell. It has the overall tone I am looking for, but I also want to be able to highlight or diff something within the image with an overlay of another Algorotoscope image.&lt;/p&gt;

&lt;p&gt;To demonstrate what I am talking about I produced three separate images to support a fictional story I wrote called &lt;a href=&quot;https://alternate.kinlane.com/2024/03/17/revealing-the-cybernetic-gears-in-our-everyday-life/&quot;&gt;Revealing the Cybernetic Gears in Our Everyday Life&lt;/a&gt;. I am looking to highlight the control Internet technology has on use using both my B.F. Skinner and &lt;a href=&quot;https://algorithmic.rotoscope.work/collections/copper-circuit/&quot;&gt;Copper Circuit Algorotoscope models&lt;/a&gt;, to bring my story about cybernetic gears to life.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-woman-walking-on-phone-store-3.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-e-bike-delivery-in-road-on-phone-combined-3.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-woman-walking-on-phone-scaffolding-3.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-woman-walking-on-phone-tan-building-3.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am fascinated with the behavioral influence of our mobile applications, which are of course API-driven. I like potential of my models to transform the landscape, but also obfuscate and transform the people I come across. I am not looking to shame anyone on their mobile phone walking the streets of New York, but I am looking to push us to think about why we do this. I spend a lot of time thinking about this as I walk to and from work in mid-town Manhattan each day.&lt;/p&gt;

&lt;p&gt;This is just my first set of composite Algorotoscope images that utilize images produced from two separate models. I am going to explore other combinations with my Birth of a Nation and John Wayne backdrops, utilizing my Gone With the Wind model for illuminating things within each image. I think I’ll explore blue, green, and purple circuits to further compliment my B.F. Skinner model. It is pushing how I think about models, but also how I go out in the world and take picture, or re-evaluate photos I have already taken in the past. I think this is the Algorotoscope dimension I will play in for a while before I spend any more time and money on training models.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Images" /><category term="Models" /><category term="Composite" /><category term="Stories" /><summary type="html">I spent some time making some new images with my B.F. Skinner Algorotoscope models the other weekend. I am pretty happy with my pallet of AI models right now. I need to get them better displays don the site, but the tones of them represent many of the emotions I am trying to illuminate right now. However, in th mean time I am now exploring how I can better overlay images produced from different Algorotoscope models. The B.F. Skinner Algorotoscope model sets the stage for many technological stories I am wanting to tell. It has the overall tone I am looking for, but I also want to be able to highlight or diff something within the image with an overlay of another Algorotoscope image.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-woman-walking-on-phone-store-3.jpg" /><media:content medium="image" url="https://kinlane-productions2.s3.amazonaws.com/algorotoscope-composites/bf-skinner-woman-walking-on-phone-store-3.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">I Am Not Well</title><link href="http://localhost:4000/2024/02/22/i-am-not-well/" rel="alternate" type="text/html" title="I Am Not Well" /><published>2024-02-22T07:00:00-05:00</published><updated>2024-02-22T07:00:00-05:00</updated><id>http://localhost:4000/2024/02/22/i-am-not-well</id><content type="html" xml:base="http://localhost:4000/2024/02/22/i-am-not-well/">&lt;p&gt;I am not well. I have something systemic wrong with me. I am not responsible for its origins, but I am responsible for perpetuating it. It is a condition that is difficult to see despite being ubiquitous throughout our lives.&lt;/p&gt;

&lt;p&gt;Most white men know something is wrong with us. The gravity is just different. We are weaker. We are more sensitive, despite being so masculine. Our physical and mental condition is not accustom to adversity and diverse voices in our head.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algorotoscope-master/birth-of-a-nation-kin-flag.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have been fed a steady diet of lies since we first could speak. Our fathers and brothers reinforced this condition, handing it down through the generations. Society has also upheld these lies, with Hollywood bringing it all home for us.&lt;/p&gt;

&lt;p&gt;I spent many years seeing this as being your disease, not mine. I have come to learn that it is our disease. There is no escaping it. You won’t be able to avoid the coming reckoning by living in a small town or building a bunker—it is too systemic.&lt;/p&gt;

&lt;p&gt;I do not see nay way out, or a way to fix this. I only see addressing it head on out in the open. Then maybe over time we can collectively overcome and begin to shift things to a healthier state. Which is why I write about it as honestly as I can.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="White Supremacy" /><category term="Toxic Masculinity" /><category term="Ignorance" /><category term="Education" /><summary type="html">I am not well. I have something systemic wrong with me. I am not responsible for its origins, but I am responsible for perpetuating it. It is a condition that is difficult to see despite being ubiquitous throughout our lives.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kinlane-productions2.s3.amazonaws.com/algorotoscope-master/birth-of-a-nation-kin-flag.jpeg" /><media:content medium="image" url="https://kinlane-productions2.s3.amazonaws.com/algorotoscope-master/birth-of-a-nation-kin-flag.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Seeing the Machine</title><link href="http://localhost:4000/2023/12/17/seeing-the-machine/" rel="alternate" type="text/html" title="Seeing the Machine" /><published>2023-12-17T07:00:00-05:00</published><updated>2023-12-17T07:00:00-05:00</updated><id>http://localhost:4000/2023/12/17/seeing-the-machine</id><content type="html" xml:base="http://localhost:4000/2023/12/17/seeing-the-machine/">&lt;p&gt;I wish I could paint the pictures I have in my head from my work as the API Evangelist. The closest I can come is projecting light and transforming images as part of my Algorotoscope Work. Some day my painting skills may get me closer to what I see emerging around us, but for right now my &lt;a href=&quot;https://algorithmic.rotoscope.work/&quot;&gt;Algorotoscope Work&lt;/a&gt; is the closest I got. I am just turning the knobs and dials on the Algorotoscope lenses I have created, until the reception gets better, and I find the images I am looking for.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-alan-turing-side.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am not using ChatGPT or some other more modern AI, I am using the previous generation of AI to steal meaning from a variety of existing art, posters, and other images, then I am applying the resulting texture transfer to photos I have taken. I am using the ways I see the Internet and algorithms are amplifying existing bias and beliefs to render and produce images that help highlight the algorithmic obfuscation occurring all around us each day via our mobile phones, laptops, desktops, televisions, automobiles, and other Internet connected devices.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-old-piano-playing-hospital.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am exploring inside what began as a piano player and has not grown into a global digital sprawl. APIs are connecting the world through each mobile phone, television, and security camera installed. Think of Algorotoscope as what the security cameras see when scanning the digital realm that exists between our physical and virtual realms. Algorotoscope is about revealing what lies in between, and shines a light on what the Internet is doing to us while it surveys us, watches us, and controls us.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-surveillance-3-camers.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Skinner was all about behaviorism, and the Internet is behaviorism run wild. It isn’t that Skinner was wrong, he was right. We all respond to positive reinforcement, and the Internet is being designed to hit on all of the right dopamine receptors with the reinforcement we desire. The government is waking up to this reality, and Lina Khan’s Federal Trade Commission is trying to find the regulatory antidote, but the machine has already grown so large and controlling, it feels almost too late.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-white-house-press-room-flag.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I don’t think we fully understand what the Internet is doing to us, let alone to our children. I think we are flying blind and willfully ignorant about the negative impacts of technology that makes lots of money. I don’t think our early concerns around Internet surveillance at all speak to the real harm being done, and I don’t think we have the vocabulary to make sense of things. I worry about what we are training the next generations of human beings to live for, and I think we are leaving a huge burden on future generations.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-grafitti-woman-bangs.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The anxiety that an online reality is giving us is going to lead to so many real world challenges. I don’t feel as equipped to deal with the world as I used, and I remember the times before. I fear how Internet induced anxiety is going to shift how our children live their lives, or more importantly, do not live their lives. The Internet is reducing us to unrecognizable digital versions of ourselves, leaving our physical bodies unhealthy, unhappy, and unable to make their way through the world.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-hiding-monster-statue.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We aren’t equipping our children to deal with the world, while as adults we are giving in to every convenience and digital cow clicker presented to us. We seem to keep lining up at the feeder for whatever is next when it comes to technology, without ever being able to examine what just happened. We are becoming feral beings in this liminal space in between the physical and online worlds, lined up on the ledge waiting for the next application to drop, and a digital feedback loop to plug into to get our fix.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-pigeons-on-ledge.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We seem to be content with grazing on the digital grass, unaware of the world that exists outside our pasture. Anytime I speak out against technology, there are some in my circle who acknowledge and share my concern, but there are many, many more who pick up their head, look annoyed, and go back to chomping on the digital grass. Being part of the digital flock seems like the norm, and people have forgotten what was before, or never knew what was before, and the Internet posture is all that we have ever had.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-sheep-grazing-grass-green.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When I close my eyes after a long day working online I feel like a bunch of cybernetic gears which I am not fully in control of. I feel like I am spinning out of control and it takes days or weeks for me to wrestle back control. I feel stretched. I feel like I am hundreds of spinning gears set in motion by my daily feeds, emails, notifications, and other digital chords that increasingly dictate who I am. I am not always in control of my behavior, and it is something that gets dictated digitally from some far off place in Silicon Valley.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-statue-woman.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I worry. I worry about our programming. Sadly, I think our programming has always been flawed, but the Internet is amplifying the worst of the worst. I sit in the middle of the machine each day, listening to the hum of the gears knowing each revolution reduces some aspect of our world to a transaction. There are few I can share this knowledge with, as the noise of the machine is just out of reach of your average person. When I do find a kindred spirit and traveler amidst the sprawling landscape of the machine I share what I can to help them in their journey.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-two-statues-rock.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is hard to share knowledge here. The machine is always watching. Always building and rebuilding its apparatus to be all seeing and knowing. The online and offline world is increasingly surveilled, and stomping out any dissent and knowledge sharing. Like the automobile, the Internet is spreading to all aspects of our lives, taking over our homes, businesses, and public spaces. The Internet demands our complete attention, which is why I take comfort inside amongst the gears. It is loud, but comforting to be outside the view of the machine, hiding within its own inner workings.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-surveillance-over-the-city.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I know I am being judged. I know I am being watched. I know that my loyalty is always in question. This is why I walk the line of being outspoken while also being quiet. I know if I fall 100% in line I will be more surveilled than I am being an outspoken engineer who knows how the machine works. Understanding how the gears turn, and how the digital belts turn the gears keeps me just out of view. I am building the machine. I know the machine. I resemble the other doozers doing what they do best. I am appalled at what I see, but since I can’t escape I keep doing my job to know how it all works.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-horrified-statue.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At some point we are going to have to pull the switch and change tracks. I know where all tracks and switches lie. I know how to get where we need to go. However, it isn’t all up to me. We are in this together. I am happy to keep knowing where the switches are for when the time comes. I am just not sure when the time is right. I am unsure if you are coming with me, or you are staying on the current line. Behaviorally we are going to need to change course, or all of this will become unattainable and consume us all.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-train-switch.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My Algorotoscope Work has given me a way to produce the lens I need to see the machine. I have a way to make all that space in between visible. I can’t reveal the algorithms for what they are. The problem is it isn’t just the algorithms. It is us. The illness isn’t in the machine. The illness is in us. The machine is just amplifying it, spreading it, and eventually rendering the illness unstoppable. My B.F. The Skinner machine learning model has provided me with the most valuable lens I have for seeing how the Internet is changing our behavior. But it is just one of a toolbox of lenses I have for evaluating the machine, and I must keep working producing new ones.&lt;/p&gt;

&lt;p&gt;I have three new filters being generated as we speak. Landing in New York City and thinking about the ways in which I was programmed to see this town has set me on the path towards these three new filters. I also have a bunch of new photos I’ve gathered traveling across the country which I’d like to hold up to the light and work my way through the lenses I have, as well as these three new ones. I feel like my Algorotosocope toolbox is getting much more equipped, yet I still have many more years of work to get where I need to. Additionally the noise of the machine has become much louder with all the foot stomping up above about generative AI, apocalypse, and all that scares us. I’ll keep working down here to do what I need, while also not losing myself entirely here within the machine.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Visual" /><category term="Machine" /><category term="Internet" /><category term="Gears" /><summary type="html">I wish I could paint the pictures I have in my head from my work as the API Evangelist. The closest I can come is projecting light and transforming images as part of my Algorotoscope Work. Some day my painting skills may get me closer to what I see emerging around us, but for right now my Algorotoscope Work is the closest I got. I am just turning the knobs and dials on the Algorotoscope lenses I have created, until the reception gets better, and I find the images I am looking for.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-alan-turing-side.jpg" /><media:content medium="image" url="https://kinlane-productions2.s3.amazonaws.com/algo-rotoscope/bf-skinner/bf-skinner-alan-turing-side.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Rebuilding the Texture Transfer ML Process</title><link href="http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process/" rel="alternate" type="text/html" title="Rebuilding the Texture Transfer ML Process" /><published>2023-01-14T07:00:00-05:00</published><updated>2023-01-14T07:00:00-05:00</updated><id>http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process</id><content type="html" xml:base="http://localhost:4000/2023/01/14/rebuilding-the-texture-transfer-ml-process/">&lt;p&gt;I had an AWS machine image that had my texture transfer process all setup. I had my AWS account compromised and someone spun up servers across almost every AWS region—my bill was pushing $25K. Luckily it was due to anything I had done and AWS didn’t charge me for any of it. However, in the shuffle from that account to a new API Evangelist specific account, my AWS machine image got lost—forcing me to have to rebuild from scratch. It is always a daunting thing to dive back into the world of TensorFlow and machine learning, but I finally made time over this last holidays season.&lt;/p&gt;

&lt;p&gt;&amp;lt;img src=https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-bus-station.jpg”” width=”100%” align=”center” title=”Bus Station”&amp;gt;&lt;/p&gt;

&lt;p&gt;I am using &lt;a href=&quot;https://github.com/lengstrom/fast-style-transfer&quot;&gt;lengstrom/fast-style-transfer&lt;/a&gt; for my texture transfer process. It really isn’t that hard to implement, but every time there is some learning curve around getting the server setup, all the TensorFlow libraries properly setup, and he keeps updating his process. This round I saw he was using Jupiter Notebooks, which I am not that interested in using, so I beat my own path forward. I am glad I did it one more time because there are a number of libraries I had to find from Archive.org to get to work, but finally I ended up with a working implementation. This time I have backed up on S3, and a local location. Ensuring that even if I lose my AMI I won’t lose the whole approach, cause I don’t think I will be able to rebuild again. The web is fragile that way. Not all of this is due to technical fragility, it is due to business fragility, disposability, and velocity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-freeway-camp.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Bus Station&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Like many things, this approach to doing texture transfer will fall victim to what is next. You already see this happening with MidJourney and Dall-E approaches to images ML. It is compelling, exciting, and superior to what came before. I am just going to fork what is and keep applying in my own way off over here in this cul-de-sac. I’ll pay attention to what is new, cause I can’t help it, but I will also be experimenting with what was. For me, the layer of expression uses this particular ML model, but also a mix of textures and photos to find the sweet spot. Honestly, this sweet spot varies depending on what my mood is, how I am feeling about the world, and whether I have the money to create a new model or experiment with  the ones I already have.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">I had an AWS machine image that had my texture transfer process all setup. I had my AWS account compromised and someone spun up servers across almost every AWS region—my bill was pushing $25K. Luckily it was due to anything I had done and AWS didn’t charge me for any of it. However, in the shuffle from that account to a new API Evangelist specific account, my AWS machine image got lost—forcing me to have to rebuild from scratch. It is always a daunting thing to dive back into the world of TensorFlow and machine learning, but I finally made time over this last holidays season.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california.jpeg" /><media:content medium="image" url="https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Rebuilding the Texture Transfer ML Process</title><link href="http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/" rel="alternate" type="text/html" title="Rebuilding the Texture Transfer ML Process" /><published>2023-01-14T07:00:00-05:00</published><updated>2023-01-14T07:00:00-05:00</updated><id>http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images</id><content type="html" xml:base="http://localhost:4000/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/">&lt;p&gt;Historically the ML models I have trained for Algorotoscope have been fairly dark in nature. I had the training wheels of the models that came with the ML process that I adopted, but then I quickly began &lt;a href=&quot;https://algorithmic.rotoscope.work/collections/the-persistence-of-memory/&quot;&gt;training on Salvador Dali and playing around with a distortion in time&lt;/a&gt;, but then found myself drowning in Nazi and Russian propaganda. I would take these models and apply them to normal everyday photos I have taken—obfuscating the darkness in the texture applied to each photo. Now, I am flipping the polarity, training more positive models, but then applying them to darker photos, experimenting with how I can further obfuscate the real world around was with digital textures. I have two models to demonstrate how I am seeing things.&lt;/p&gt;

&lt;h2 id=&quot;train-travel-to-california&quot;&gt;Train Travel to California&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/california-travel-by-train/california-travel-by-train.jpeg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Train Travel to California&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;applied-to-train-travel-in-california&quot;&gt;Applied to Train Travel in California&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/california-travel-by-train/california-travel-by-train-freeway-interchange-fence.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Applied to Train Travel in California&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;oakland-california-bridge&quot;&gt;Oakland, California Bridge&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california.jpeg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Oakland, California Bridge&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;applied-to-oakland-california-bridge&quot;&gt;Applied to Oakland, California Bridge&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-freeway-camp.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Applied to Oakland, California Bridge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Early on in my exploration I wanted to take previous propaganda and make pictures distorted in an algorithmic way using this old world images. Now I am feeling like I want to take old world marketing and use it to distort in an algorithmic way the photos I am taking as I walk around my community, state, and wider. As always, I have no idea where I am going with this. I am just exploring machine learning in my own way, taking photographs, and playing around with how I can mix the two to say something.&lt;/p&gt;

&lt;p&gt;I historically have used my images in my storytelling on API Evangelist. Using everyday photos that are obfuscated with some pretty dark propaganda from our pasts to quietly show the distortion field that is being created using APIs. These new round of photos most likely will have no place in my API Evangelist storytelling, pushing me to use on my Alternate channels, Kin Lane, and other places I am expressing myself online. I am happy that I finally have my text transfer ML process back working, and I also now have two more models to play with when it comes to applying to photos. I’ll keep learning where these photos translate best. How the clouds, rocks, buildings, people, and other real world objects get obfuscated, and I’ll work to take photos that bring this out in new and interesting ways.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">Historically the ML models I have trained for Algorotoscope have been fairly dark in nature. I had the training wheels of the models that came with the ML process that I adopted, but then I quickly began training on Salvador Dali and playing around with a distortion in time, but then found myself drowning in Nazi and Russian propaganda. I would take these models and apply them to normal everyday photos I have taken—obfuscating the darkness in the texture applied to each photo. Now, I am flipping the polarity, training more positive models, but then applying them to darker photos, experimenting with how I can further obfuscate the real world around was with digital textures. I have two models to demonstrate how I am seeing things.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://algorithmic.rotoscope.work/images/collections/bf-skinner-behaviorism/bf-skinner-old-piano-playing-hospital.jpg" /><media:content medium="image" url="https://algorithmic.rotoscope.work/images/collections/bf-skinner-behaviorism/bf-skinner-old-piano-playing-hospital.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Why I Do Algorotoscope</title><link href="http://localhost:4000/2023/01/14/why-i-do-algorotoscope/" rel="alternate" type="text/html" title="Why I Do Algorotoscope" /><published>2023-01-14T07:00:00-05:00</published><updated>2023-01-14T07:00:00-05:00</updated><id>http://localhost:4000/2023/01/14/why-i-do-algorotoscope</id><content type="html" xml:base="http://localhost:4000/2023/01/14/why-i-do-algorotoscope/">&lt;p&gt;I started Algorotoscope as an escape from the 2016 election. I wanted to learn more about machine learning and TensorFlow while simultaneously wanting to escape the world. Once I established a working model I wanted to get out in the world more to take photos and videos. I lost the Amazon Machine Image (AMI) for my Algorotoscope process as part of switching accounts, so I haven’t produced anything new in a while. I am just working from images I have already applied my models. Now I have my AMI and a newly optimized process for producing texture transfer models, but also applying them to large amounts of images. So, I am looking to continue my exploration, with an emphasis on being out in the world taking photos that speak to the characteristics of each ML model, applying to my new and old images, and telling stories that support them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/bf-skinner-behaviorism/bf-skinner-alan-turing-side.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;BF Skinner Turing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If I have to reaffirm why I do Algorotoscope. It is less about learning ML or escaping the world now. I’d say it is about exploring the world. Finding relevant images I can train models on and then going out in the world to take photographs and film videos that leverage the strengths of each of the models. Visually and contextually. I just don’t want to be a one trick pony with the dark models applied to every day images. I want to come at it from multiple dimensions, reflecting on both the physical and digital spaces we live in. The overlap of the two. The obfuscation. The reality distortion field that consumes us online, but then also colors, paints, and texturizes the actual world around us.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/copper-circuit/christianity.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Polishing Technological Religion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand where I am coming from, it helps to understand how I see the world around us. I see APIs. I see millions of digital interfaces distorting our worlds. Some recent examples can be found with Facebook, Twitter, and the 2016 election, or how our legal system is being automated with algorithms Human Resources for our companies are flowing through these pipes, deciding who they hire and who they don’t. I see our world flowing through APIs each day, and I see how existing biases are being codified in these pipes and gears that our powering not just our online lives, but our physical world. This is why I publish Algorotoscope images with each post you see on API Evangelist—it is a visual representation of the space I’m telling a story in. I need a steady stream of images that are relevant to how I am describing the knobs, levers, and gears of the machine, but also with some context of the bias being baked in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algorithmic.rotoscope.work/images/collections/nazi-poster/downtown-usa.jpg&quot; width=&quot;100%&quot; align=&quot;center&quot; title=&quot;Small Town Propaganda&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Algorotoscope is how I show how I see the digital world consuming our physical worlds. Consuming, then dictating and shaping our lives. I don’t know how else to illustrate it. I am a software engineer, architect, and storyteller. I can’t draw this shit. I can only express it using ML and photographing—two things I cam capable of doing in this moment. The Algorotoscope images are the closest I can come to articulating what I am seeing distort and obfuscate our lives. It is the static in our heads after we’ve been online too long. It visualizes why we have a lack of control over our lives right now. It provides a quick snapshot of just one more moment where a moment of our lives is being reduced to a transaction.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Models" /><category term="Machine Learning" /><category term="Amazon Web Services" /><category term="Texture Transfer" /><summary type="html">I started Algorotoscope as an escape from the 2016 election. I wanted to learn more about machine learning and TensorFlow while simultaneously wanting to escape the world. Once I established a working model I wanted to get out in the world more to take photos and videos. I lost the Amazon Machine Image (AMI) for my Algorotoscope process as part of switching accounts, so I haven’t produced anything new in a while. I am just working from images I have already applied my models. Now I have my AMI and a newly optimized process for producing texture transfer models, but also applying them to large amounts of images. So, I am looking to continue my exploration, with an emphasis on being out in the world taking photos that speak to the characteristics of each ML model, applying to my new and old images, and telling stories that support them.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/copper-circuit-kin-chesapeake.jpg" /><media:content medium="image" url="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/copper-circuit-kin-chesapeake.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Landing on a Stable Instance of Tensorflow on Amazon to Train Models</title><link href="http://localhost:4000/2020/05/16/landing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/" rel="alternate" type="text/html" title="Landing on a Stable Instance of Tensorflow on Amazon to Train Models" /><published>2020-05-16T08:00:00-04:00</published><updated>2020-05-16T08:00:00-04:00</updated><id>http://localhost:4000/2020/05/16/landing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models</id><content type="html" xml:base="http://localhost:4000/2020/05/16/landing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/">&lt;p&gt;I have bounced around quite a bit from the original Tensorflow model I was using when I started this work. Algorithmia had done a lot of the heavy lifting for me, but that original Amazon Web Services machine image eventually fell into disrepair and I couldn’t operate it anymore. For the last couple of years I bounced around trying different text transfer services, and kicking the tires on a variety of Tensorflow models that were cheaper to operate, but none of them produced the same results as some of the earlier models I had developed. To make things worse, I had accidentally deleted some of my earlier models, leaving me pretty determined to produce my own machine learning model.&lt;/p&gt;

&lt;p&gt;Well, after much experimentation I finally created one that works as well as the first model I had. Actually, it is the same model, I just managed to find the original source of how to build it, followed their instructions and then I was able to get up and going. I will do a formal write-up on it citing my sources, and walking through how I did what I did. I just haven’t had time to write it up – each time I go to operate it and apply to my current batch of images I have to retrain myself on how it all works. Eventually I’ve carve out enough time to write up an on-boarding document for it, and publish it here on the blog for this work.&lt;/p&gt;

&lt;p&gt;I’m just looking to draw a line in the sand here on the updates for when I finally got full control over how I train my models. So when I’m looking back I can better understand what images were from the first batch of models, what came the years in between, and what has been produced as part of this new process. There are distinct differences between each of the phases, how I trained each model, the length of time, and ultimately how I chose to apply them. Back in 2016 I had the money to run a GPU server for two months straight. I don’t have those kind of resources today, but I’m regularly funneling money into it whenever I can. Then step back occasionally and organized what I’ve produced into collections that I can use to showcase what I am seeing in my head when it comes to how AI, ML, APIs, and the Internet are being used to distort not just our online worlds, but our physical worlds as well.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Collections" /><category term="Models" /><summary type="html">I have bounced around quite a bit from the original Tensorflow model I was using when I started this work. Algorithmia had done a lot of the heavy lifting for me, but that original Amazon Web Services machine image eventually fell into disrepair and I couldn’t operate it anymore. For the last couple of years I bounced around trying different text transfer services, and kicking the tires on a variety of Tensorflow models that were cheaper to operate, but none of them produced the same results as some of the earlier models I had developed. To make things worse, I had accidentally deleted some of my earlier models, leaving me pretty determined to produce my own machine learning model.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /><media:content medium="image" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Adding Some Collections of My Algorotoscope Work</title><link href="http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work/" rel="alternate" type="text/html" title="Adding Some Collections of My Algorotoscope Work" /><published>2017-07-15T08:00:00-04:00</published><updated>2017-07-15T08:00:00-04:00</updated><id>http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work</id><content type="html" xml:base="http://localhost:4000/2017/07/15/adding-some-collections-of-my-algorotoscope-work/">&lt;p&gt;After organizing a couple of the latest batches of images I produced as part of my algorotoscope work I wanted to step back and see what I had made. I wanted to get better at organizing some of the more interesting images I had produced using specific Tensorflow machine learning models. Much of my work is pretty random without much connection between the image I trained my models on and the images I applied my models to–they are just making for some interesting colors and textures. However, along the way I have been trying to squeeze more meaning as well as texture and colors out of the models I am training as well as the photos I take, and apply these models to.&lt;/p&gt;

&lt;p&gt;After reviewing my work I came across five separate filters I feel like are beginning to get at what I am seeing when it comes to algorithmic distortion on the web. Providing a collection of images that I feel represent this work well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/the-persistence-of-memory/&quot;&gt;&lt;strong&gt;The Persistence of Memory&lt;/strong&gt;&lt;/a&gt; - One of the first ML models I trained using Dali’s thought provoking work.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/copper-circuit/&quot;&gt;&lt;strong&gt;Copper Circuit&lt;/strong&gt;&lt;/a&gt; - One of my earlier models that I trained by leaving the world of art and using objects.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/nazi-invasion/&quot;&gt;&lt;strong&gt;Nazi Invasion&lt;/strong&gt;&lt;/a&gt; - Produced during a darker period after the Charlottesville rally rattled our world.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/nazi-poster/&quot;&gt;&lt;strong&gt;Nazi Poster&lt;/strong&gt;&lt;/a&gt; - A continuation of that work showing how the web is being used to promote hate and fear.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/collections/russian-propaganda/&quot;&gt;&lt;strong&gt;Russian Propaganda&lt;/strong&gt;&lt;/a&gt; - Looking at how Russian propaganda is disseminated online, all the way up to the White House.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have several other collections simmering behind the scenes. I’ve bee producing some interesting photos and continue to generate ML models from interesting subjects.  I’ll add photos to these collections as new ones emerge, and I’ll add other collections when I get enough photos to round off each collection, and enough words to tell the story. It just makes me happy to surface some of these older images I had created, but were beginning to get buried by each newer wave of work.&lt;/p&gt;

&lt;p&gt;I am not happy with the majority of what I am producing, however I do have full control over how I train my models, as well as apply them to images–I haven’t for a couple of years now. This will help stabilize my work, helping me mine specific topics, time periods, and imagery. Currently I am looking for ways that I can help visualize how technology is being used to distort and provide disinformation when it comes to the COVID-19 pandemic. It will take me a while to focus in on some meaningful images that I can use, as well as take some new photos of common aspects of the world we live in right now.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Kin Lane" /><category term="Algorotoscope" /><summary type="html">After organizing a couple of the latest batches of images I produced as part of my algorotoscope work I wanted to step back and see what I had made. I wanted to get better at organizing some of the more interesting images I had produced using specific Tensorflow machine learning models. Much of my work is pretty random without much connection between the image I trained my models on and the images I applied my models to–they are just making for some interesting colors and textures. However, along the way I have been trying to squeeze more meaning as well as texture and colors out of the models I am training as well as the photos I take, and apply these models to.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /><media:content medium="image" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Showing What Algorithmic Influence On Markets Leaves Out</title><link href="http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/" rel="alternate" type="text/html" title="Showing What Algorithmic Influence On Markets Leaves Out" /><published>2017-07-15T08:00:00-04:00</published><updated>2017-07-15T08:00:00-04:00</updated><id>http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out</id><content type="html" xml:base="http://localhost:4000/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/">&lt;p&gt;I’ve been playing with different ways of visualizing the impact that algorithms are making on our lives. How they are being &lt;a href=&quot;http://kinlane.com/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;used to distort the immigration debate&lt;/a&gt;, and &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;how the current administration is being influenced and p0wned by Russian propaganda&lt;/a&gt;. I find shedding light on how algorithms are directly influencing a variety of conversations using machine learning a fun pastime. I’m also interested in finding ways to shine a light on what gets filtered out, omitted, censored, or completely forgotten by algorithms, and their authors.&lt;/p&gt;

&lt;p&gt;One of my latest filters I’ve trained using TensorFlow is called “Feed the People”. It is an early 20th century Soviet propaganda poster that I do not know much history behind, but I feel provides a compelling point, while also providing an attractive and usable color palette and textures–I will have to do more research on the back story. I took this propaganda poster and trained a TensorFlow machine learning model for about 24 hours on an AWS EC2 GPU instance, which cost me about $18.00 for the entire process–leaving me with a ML model I can apply to any image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg&quot; align=&quot;right&quot; width=&quot;98%&quot; style=&quot;padding: 15px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once I had my trained machine learning model I applied to a handful of images, including one I took of the economist Adam Smith statue in Edinburgh, Scotland–which interestingly was commissioned by &lt;a href=&quot;https://en.wikipedia.org/wiki/Adam_Smith_Institute&quot;&gt;the Adam Smith Institute (ASI)&lt;/a&gt;, a neoliberal (formerly libertarian) think tank and lobbying group based in the United Kingdom, named after Adam Smith, a Scottish moral philosopher and classical economist in 2003. Taking the essence of the “feed the people” propaganda and algorithmically transferring it an image of the famous economist from the 18th century that was installed on the city streets by a neoliberal think tank in 2003.&lt;/p&gt;

&lt;p&gt;I’m super fascinated by how algorithms influence markets, from high speed trading, all the way to how stories about markets are spread on Facebook by investors, and libertarian and neoliberal influencers. Algorithms are being used to distort, contort, p0wn, influence and create new markets. I am continuing to trying to understand how propaganda and ideology is influencing these algorithms, but more importantly highlighting the conversations, and people that are ultimately left behind in the cracks as algorithms continue to consume our digital and physical worlds, and disrupt everything along the way.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Kin Lane" /><category term="Algorotoscope" /><summary type="html">I’ve been playing with different ways of visualizing the impact that algorithms are making on our lives. How they are being used to distort the immigration debate, and how the current administration is being influenced and p0wned by Russian propaganda. I find shedding light on how algorithms are directly influencing a variety of conversations using machine learning a fun pastime. I’m also interested in finding ways to shine a light on what gets filtered out, omitted, censored, or completely forgotten by algorithms, and their authors.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" /><media:content medium="image" url="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/markets/feed-the-people-economics.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Highlighting Algorithmic Transparency Using My Algorotoscope Work</title><link href="http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/" rel="alternate" type="text/html" title="Highlighting Algorithmic Transparency Using My Algorotoscope Work" /><published>2017-06-28T08:00:00-04:00</published><updated>2017-06-28T08:00:00-04:00</updated><id>http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.%20markdown</id><content type="html" xml:base="http://localhost:4000/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/">&lt;p&gt;&lt;a href=&quot;http://algorithmic.rotoscope.work/updates/&quot; title=&quot;Algorithmic Rotoscope&quot;&gt;I started doing my algorotoscope work to better understand machine learning&lt;/a&gt;. I needed a hands-on project that would allow me to play with two overlapping slices of the machine learning pie–working with images and video. I wanted to understand what people meant when they said texture transfer or object recognition, and quantify the differences between machine learning providers, pulling the curtain back a little on a portion of machine learning, helping establish some transparency and observability.&lt;/p&gt;

&lt;p&gt;Algorotoscope allows me shine a light on machine learning, while also shining a light on the world of algorithms. I’m still learning what is possible, but the motivations behind &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/06/algorithmic-reflections-on-the-immigration-debate/&quot;&gt;my Ellis Island Nazi Poster reflection&lt;/a&gt;, and &lt;a href=&quot;http://algorithmic.rotoscope.work/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/&quot;&gt;my White House Russian propaganda leaflet snapshot&lt;/a&gt; are meant to help me understand machine learning texture transfer models, and apply them to images in a way that helps demonstrate how algorithms are obfuscating the physical and digital world around us. Showcasing that algorithms are being used to distort the immigration debate, our elections, and almost every other aspect of our professional and personal lives.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://s3.amazonaws.com/kinlane-productions2/algorotoscope/stories/crypto-machine-bletchley_copper_circuit.png&quot; width=&quot;90%&quot; style=&quot;padding: 25px;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I understand technology by using it. Black box algorithms seem to be indistinguishable from magic for many folks, while they scare the hell out of me. Not because they contain magic, but because they contain exploitation, bias, corruption, privacy, and security nightmares. It is important to me that we understand the levers, knobs, dials, and gears behind algorithms. I am looking to use my algorotoscope work help reduce the distortion field that often surrounds algorithms, and how their various incarnations are being marketed. I want my readers to understand that nothing they read, no image they see, or video they watch is free of algorithmic influence, and that algorithms are making the decision about what you see, as well as what we do not see.&lt;/p&gt;

&lt;p&gt;Algorotscope is all about using machine learning to help us visualize the impact that algorithms are making on our world. I have no idea where the work is headed, except that I will keep working to generate relevant machine learning models trained on relevant images, then experiment with the application of these models as filters on images and video in a way that tells a story about how algorithms are distorting our world, and shifting how we view things both on and offline. I’m looking to move my &lt;a href=&quot;http://apievangelist.com&quot;&gt;API Evangelist storytelling to use 100% algorotscope images&lt;/a&gt;, as I keep scratching the surface of how algorithms are invading our lives via the web, devices, and everyday objects.&lt;/p&gt;</content><author><name>Kin Lane</name></author><category term="Algorotoscope" /><category term="Algorithms" /><summary type="html">I started doing my algorotoscope work to better understand machine learning. I needed a hands-on project that would allow me to play with two overlapping slices of the machine learning pie–working with images and video. I wanted to understand what people meant when they said texture transfer or object recognition, and quantify the differences between machine learning providers, pulling the curtain back a little on a portion of machine learning, helping establish some transparency and observability.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://algorithmic.rotoscope.work/updates/ &quot;Algorithmic Rotoscope" /><media:content medium="image" url="http://algorithmic.rotoscope.work/updates/ &quot;Algorithmic Rotoscope" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>