<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kin Lane</title>
  <updated>2023-12-17T07:00:00Z</updated>
  <link rel="self" href="http://localhost:4000/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>http://localhost:4000/atom.xml</id>
	<entry>
    <title>Seeing the Machine</title>
    <link href="http://apievangelist.com/2023/12/17/seeing-the-machine/"/>
    <updated>2023-12-17T07:00:00Z</updated>
    <content><![CDATA[I wish I could paint the pictures I have in my head from my work as the API Evangelist. The closest I can come is project light and transforming images as part of my Algorotoscope Work. Some day my painting skills may get me closer to what I see emerging around us, but for right now my Algorotoscope Work is the closest I got. I am just turning the knobs and dials on the Algorotoscope lens I’ve created, until the reception gets better, and I find the image I am looking for. I am not using ChatGPT or some other more modern AI, I am using the previous generation of AI to steal meaning from a variety of existing art, posters, and other images, then I am applying the resulting texture transfer to photos I have taken. I am using the ways I see the Internet and algorithms are amplifying existing bias and beliefs to render and produce images that help highlight the algorithmic obfuscation occurring all around us each day via our mobile phones, laptops, desktops, televisions, automobiles, and other Internet connected devices. I am exploring inside what began as a piano player and has not grown into a global digital sprawl. APIs are connecting the world through each mobile phone, television, and security camera installed. Think of Algorotoscope as what the security cameras see when scanning the digital realm that exists between our physical and virtual realms. Algorotoscope is about revealing what lies in between, and shines a light on what the Internet is doing to us while it surveys us, watches us, and controls us. Skinner was all about behaviorism, and the Internet is behaviorism run wild. It isn’t that Skinner was wrong, he was right. We all respond to positive reinforcement, and the Internet is being designed to hit on all of the right dopamine receptors with the reinforcement we desire. The government is waking up to this reality, and Lina Khan’s Federal Trade Commission is trying to find the regulatory antidote, but the machine has already grown so large and controlling, it feels almost too late. I don’t think we fully understand what the Internet is doing to us, let alone to our children. I think we are flying blind and willfully ignorant about the negative impacts of technology that makes lots of money. I don’t think our early concerns around Internet surveillance at all speak to the real harm being done, and I don’t think we have the vocabulary to make sense of things. I worry about what we are training the next generations of human beings to live for, and I think we are leaving a huge burden on future generations. The anxiety that an online reality is giving us is going to lead to so many real world challenges. I don’t feel as equipped to deal with the world as I used, and I remember the times before. I fear how Internet induced anxiety is going to shift how our children live...]]></content>
    <id>http://apievangelist.com/2023/12/17/seeing-the-machine/</id>
  </entry><entry>
    <title>Why I Do Algorotoscope</title>
    <link href="http://apievangelist.com/2023/01/14/why-i-do-algorotoscope/"/>
    <updated>2023-01-14T07:00:00Z</updated>
    <content><![CDATA[I started Algorotoscope as an escape from the 2016 election. I wanted to learn more about machine learning and TensorFlow while simultaneously wanting to escape the world. Once I established a working model I wanted to get out in the world more to take photos and videos. I lost the Amazon Machine Image (AMI) for my Algorotoscope process as part of switching accounts, so I haven’t produced anything new in a while. I am just working from images I have already applied my models. Now I have my AMI and a newly optimized process for producing texture transfer models, but also applying them to large amounts of images. So, I am looking to continue my exploration, with an emphasis on being out in the world taking photos that speak to the characteristics of each ML model, applying to my new and old images, and telling stories that support them. If I have to reaffirm why I do Algorotoscope. It is less about learning ML or escaping the world now. I’d say it is about exploring the world. Finding relevant images I can train models on and then going out in the world to take photographs and film videos that leverage the strengths of each of the models. Visually and contextually. I just don’t want to be a one trick pony with the dark models applied to every day images. I want to come at it from multiple dimensions, reflecting on both the physical and digital spaces we live in. The overlap of the two. The obfuscation. The reality distortion field that consumes us online, but then also colors, paints, and texturizes the actual world around us. To understand where I am coming from, it helps to understand how I see the world around us. I see APIs. I see millions of digital interfaces distorting our worlds. Some recent examples can be found with Facebook, Twitter, and the 2016 election, or how our legal system is being automated with algorithms Human Resources for our companies are flowing through these pipes, deciding who they hire and who they don’t. I see our world flowing through APIs each day, and I see how existing biases are being codified in these pipes and gears that our powering not just our online lives, but our physical world. This is why I publish Algorotoscope images with each post you see on API Evangelist—it is a visual representation of the space I’m telling a story in. I need a steady stream of images that are relevant to how I am describing the knobs, levers, and gears of the machine, but also with some context of the bias being baked in. Algorotoscope is how I show how I see the digital world consuming our physical worlds. Consuming, then dictating and shaping our lives. I don’t know how else to illustrate it. I am a software engineer, architect, and storyteller. I can’t draw this shit. I can only express it using ML and photographing—two things I cam capable of...]]></content>
    <id>http://apievangelist.com/2023/01/14/why-i-do-algorotoscope/</id>
  </entry><entry>
    <title>Rebuilding the Texture Transfer ML Process</title>
    <link href="http://apievangelist.com/2023/01/14/rebuilding-the-texture-transfer-ml-process/"/>
    <updated>2023-01-14T07:00:00Z</updated>
    <content><![CDATA[I had an AWS machine image that had my texture transfer process all setup. I had my AWS account compromised and someone spun up servers across almost every AWS region—my bill was pushing $25K. Luckily it was due to anything I had done and AWS didn’t charge me for any of it. However, in the shuffle from that account to a new API Evangelist specific account, my AWS machine image got lost—forcing me to have to rebuild from scratch. It is always a daunting thing to dive back into the world of TensorFlow and machine learning, but I finally made time over this last holidays season.

&amp;lt;img src=https://algorithmic.rotoscope.work/images/collections/oakland-california/oakland-california-bus-station.jpg”” width=”100%” align=”center” title=”Bus Station”&amp;gt;

I am using lengstrom/fast-style-transfer for my texture transfer process. It really isn’t that hard to implement, but every time there is some learning curve around getting the server setup, all the TensorFlow libraries properly setup, and he keeps updating his process. This round I saw he was using Jupiter Notebooks, which I am not that interested in using, so I beat my own path forward. I am glad I did it one more time because there are a number of libraries I had to find from Archive.org to get to work, but finally I ended up with a working implementation. This time I have backed up on S3, and a local location. Ensuring that even if I lose my AMI I won’t lose the whole approach, cause I don’t think I will be able to rebuild again. The web is fragile that way. Not all of this is due to technical fragility, it is due to business fragility, disposability, and velocity.



Like many things, this approach to doing texture transfer will fall victim to what is next. You already see this happening with MidJourney and Dall-E approaches to images ML. It is compelling, exciting, and superior to what came before. I am just going to fork what is and keep applying in my own way off over here in this cul-de-sac. I’ll pay attention to what is new, cause I can’t help it, but I will also be experimenting with what was. For me, the layer of expression uses this particular ML model, but also a mix of textures and photos to find the sweet spot. Honestly, this sweet spot varies depending on what my mood is, how I am feeling about the world, and whether I have the money to create a new model or experiment with  the ones I already have.
]]></content>
    <id>http://apievangelist.com/2023/01/14/rebuilding-the-texture-transfer-ml-process/</id>
  </entry><entry>
    <title>Rebuilding the Texture Transfer ML Process</title>
    <link href="http://apievangelist.com/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/"/>
    <updated>2023-01-14T07:00:00Z</updated>
    <content><![CDATA[Historically the ML models I have trained for Algorotoscope have been fairly dark in nature. I had the training wheels of the models that came with the ML process that I adopted, but then I quickly began training on Salvador Dali and playing around with a distortion in time, but then found myself drowning in Nazi and Russian propaganda. I would take these models and apply them to normal everyday photos I have taken—obfuscating the darkness in the texture applied to each photo. Now, I am flipping the polarity, training more positive models, but then applying them to darker photos, experimenting with how I can further obfuscate the real world around was with digital textures. I have two models to demonstrate how I am seeing things.

Train Travel to California


Applied to Train Travel in California


Oakland, California Bridge


Applied to Oakland, California Bridge


Early on in my exploration I wanted to take previous propaganda and make pictures distorted in an algorithmic way using this old world images. Now I am feeling like I want to take old world marketing and use it to distort in an algorithmic way the photos I am taking as I walk around my community, state, and wider. As always, I have no idea where I am going with this. I am just exploring machine learning in my own way, taking photographs, and playing around with how I can mix the two to say something.

I historically have used my images in my storytelling on API Evangelist. Using everyday photos that are obfuscated with some pretty dark propaganda from our pasts to quietly show the distortion field that is being created using APIs. These new round of photos most likely will have no place in my API Evangelist storytelling, pushing me to use on my Alternate channels, Kin Lane, and other places I am expressing myself online. I am happy that I finally have my text transfer ML process back working, and I also now have two more models to play with when it comes to applying to photos. I’ll keep learning where these photos translate best. How the clouds, rocks, buildings, people, and other real world objects get obfuscated, and I’ll work to take photos that bring this out in new and interesting ways.
]]></content>
    <id>http://apievangelist.com/2023/01/14/flipping-the-polarity-of-the-relationship-between-models-and-images/</id>
  </entry><entry>
    <title>Landing on a Stable Instance of Tensorflow on Amazon to Train Models</title>
    <link href="http://apievangelist.com/2020/05/16/landiing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/"/>
    <updated>2020-05-16T08:00:00Z</updated>
    <content><![CDATA[I have bounced around quite a bit from the original Tensorflow model I was using when I started this work. Algorithmia had done a lot of the heavy lifting for me, but that original Amazon Web Services machine image eventually fell into disrepair and I couldn’t operate it anymore. For the last couple of years I bounced around trying different text transfer services, and kicking the tires on a variety of Tensorflow models that were cheaper to operate, but none of them produced the same results as some of the earlier models I had developed. To make things worse, I had accidentally deleted some of my earlier models, leaving me pretty determined to produce my own machine learning model.

Well, after much experimentation I finally created one that works as well as the first model I had. Actually, it is the same model, I just managed to find the original source of how to build it, followed their instructions and then I was able to get up and going. I will do a formal write-up on it citing my sources, and walking through how I did what I did. I just haven’t had time to write it up – each time I go to operate it and apply to my current batch of images I have to retrain myself on how it all works. Eventually I’ve carve out enough time to write up an on-boarding document for it, and publish it here on the blog for this work.

I’m just looking to draw a line in the sand here on the updates for when I finally got full control over how I train my models. So when I’m looking back I can better understand what images were from the first batch of models, what came the years in between, and what has been produced as part of this new process. There are distinct differences between each of the phases, how I trained each model, the length of time, and ultimately how I chose to apply them. Back in 2016 I had the money to run a GPU server for two months straight. I don’t have those kind of resources today, but I’m regularly funneling money into it whenever I can. Then step back occasionally and organized what I’ve produced into collections that I can use to showcase what I am seeing in my head when it comes to how AI, ML, APIs, and the Internet are being used to distort not just our online worlds, but our physical worlds as well.
]]></content>
    <id>http://apievangelist.com/2020/05/16/landiing-on-a-stable-instance-of-tensorflow-on-amazon-to-train-models/</id>
  </entry><entry>
    <title>Adding Some Collections of My Algorotoscope Work</title>
    <link href="http://apievangelist.com/2017/07/15/adding-some-collections-of-my-algorotoscope-work/"/>
    <updated>2017-07-15T08:00:00Z</updated>
    <content><![CDATA[After organizing a couple of the latest batches of images I produced as part of my algorotoscope work I wanted to step back and see what I had made. I wanted to get better at organizing some of the more interesting images I had produced using specific Tensorflow machine learning models. Much of my work is pretty random without much connection between the image I trained my models on and the images I applied my models to–they are just making for some interesting colors and textures. However, along the way I have been trying to squeeze more meaning as well as texture and colors out of the models I am training as well as the photos I take, and apply these models to.

After reviewing my work I came across five separate filters I feel like are beginning to get at what I am seeing when it comes to algorithmic distortion on the web. Providing a collection of images that I feel represent this work well.


  The Persistence of Memory - One of the first ML models I trained using Dali’s thought provoking work.
  Copper Circuit - One of my earlier models that I trained by leaving the world of art and using objects.
  Nazi Invasion - Produced during a darker period after the Charlottesville rally rattled our world.
  Nazi Poster - A continuation of that work showing how the web is being used to promote hate and fear.
  Russian Propaganda - Looking at how Russian propaganda is disseminated online, all the way up to the White House.


I have several other collections simmering behind the scenes. I’ve bee producing some interesting photos and continue to generate ML models from interesting subjects.  I’ll add photos to these collections as new ones emerge, and I’ll add other collections when I get enough photos to round off each collection, and enough words to tell the story. It just makes me happy to surface some of these older images I had created, but were beginning to get buried by each newer wave of work.

I am not happy with the majority of what I am producing, however I do have full control over how I train my models, as well as apply them to images–I haven’t for a couple of years now. This will help stabilize my work, helping me mine specific topics, time periods, and imagery. Currently I am looking for ways that I can help visualize how technology is being used to distort and provide disinformation when it comes to the COVID-19 pandemic. It will take me a while to focus in on some meaningful images that I can use, as well as take some new photos of common aspects of the world we live in right now.
]]></content>
    <id>http://apievangelist.com/2017/07/15/adding-some-collections-of-my-algorotoscope-work/</id>
  </entry><entry>
    <title>Showing What Algorithmic Influence On Markets Leaves Out</title>
    <link href="http://apievangelist.com/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/"/>
    <updated>2017-07-15T08:00:00Z</updated>
    <content><![CDATA[I’ve been playing with different ways of visualizing the impact that algorithms are making on our lives. How they are being used to distort the immigration debate, and how the current administration is being influenced and p0wned by Russian propaganda. I find shedding light on how algorithms are directly influencing a variety of conversations using machine learning a fun pastime. I’m also interested in finding ways to shine a light on what gets filtered out, omitted, censored, or completely forgotten by algorithms, and their authors.

One of my latest filters I’ve trained using TensorFlow is called “Feed the People”. It is an early 20th century Soviet propaganda poster that I do not know much history behind, but I feel provides a compelling point, while also providing an attractive and usable color palette and textures–I will have to do more research on the back story. I took this propaganda poster and trained a TensorFlow machine learning model for about 24 hours on an AWS EC2 GPU instance, which cost me about $18.00 for the entire process–leaving me with a ML model I can apply to any image.



Once I had my trained machine learning model I applied to a handful of images, including one I took of the economist Adam Smith statue in Edinburgh, Scotland–which interestingly was commissioned by the Adam Smith Institute (ASI), a neoliberal (formerly libertarian) think tank and lobbying group based in the United Kingdom, named after Adam Smith, a Scottish moral philosopher and classical economist in 2003. Taking the essence of the “feed the people” propaganda and algorithmically transferring it an image of the famous economist from the 18th century that was installed on the city streets by a neoliberal think tank in 2003.

I’m super fascinated by how algorithms influence markets, from high speed trading, all the way to how stories about markets are spread on Facebook by investors, and libertarian and neoliberal influencers. Algorithms are being used to distort, contort, p0wn, influence and create new markets. I am continuing to trying to understand how propaganda and ideology is influencing these algorithms, but more importantly highlighting the conversations, and people that are ultimately left behind in the cracks as algorithms continue to consume our digital and physical worlds, and disrupt everything along the way.
]]></content>
    <id>http://apievangelist.com/2017/07/15/showing-what-algorithmic-influence-on-markets-leave-out/</id>
  </entry><entry>
    <title>Highlighting Algorithmic Transparency Using My Algorotoscope Work</title>
    <link href="http://apievangelist.com/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/"/>
    <updated>2017-06-28T08:00:00Z</updated>
    <content><![CDATA[I started doing my algorotoscope work to better understand machine learning. I needed a hands-on project that would allow me to play with two overlapping slices of the machine learning pie–working with images and video. I wanted to understand what people meant when they said texture transfer or object recognition, and quantify the differences between machine learning providers, pulling the curtain back a little on a portion of machine learning, helping establish some transparency and observability.

Algorotoscope allows me shine a light on machine learning, while also shining a light on the world of algorithms. I’m still learning what is possible, but the motivations behind my Ellis Island Nazi Poster reflection, and my White House Russian propaganda leaflet snapshot are meant to help me understand machine learning texture transfer models, and apply them to images in a way that helps demonstrate how algorithms are obfuscating the physical and digital world around us. Showcasing that algorithms are being used to distort the immigration debate, our elections, and almost every other aspect of our professional and personal lives.

I understand technology by using it. Black box algorithms seem to be indistinguishable from magic for many folks, while they scare the hell out of me. Not because they contain magic, but because they contain exploitation, bias, corruption, privacy, and security nightmares. It is important to me that we understand the levers, knobs, dials, and gears behind algorithms. I am looking to use my algorotoscope work help reduce the distortion field that often surrounds algorithms, and how their various incarnations are being marketed. I want my readers to understand that nothing they read, no image they see, or video they watch is free of algorithmic influence, and that algorithms are making the decision about what you see, as well as what we do not see.

Algorotscope is all about using machine learning to help us visualize the impact that algorithms are making on our world. I have no idea where the work is headed, except that I will keep working to generate relevant machine learning models trained on relevant images, then experiment with the application of these models as filters on images and video in a way that tells a story about how algorithms are distorting our world, and shifting how we view things both on and offline. I’m looking to move my API Evangelist storytelling to use 100% algorotscope images, as I keep scratching the surface of how algorithms are invading our lives via the web, devices, and everyday objects.
]]></content>
    <id>http://apievangelist.com/2017/06/28/highlighting-algorithmic-transparency-using-my-algorotoscope-work.-markdown/</id>
  </entry><entry>
    <title>Why Would People Want Fine Art Trained Machine Learning Models</title>
    <link href="http://apievangelist.com/2017/03/08/why-would-people-want-fine-art-trained-machine-learning-models/"/>
    <updated>2017-03-08T07:00:00Z</updated>
    <content><![CDATA[I'm spending time on my algorithmic rotoscope work, and thinking about how the machine learning style textures I've been marking can be put to use. I'm trying to see things from different vantage points and develop a better understanding of how texture styles can be put to use in the regular world. I am enjoying using image style filters in my writing. It gives me kind of a gamified layer to my photography and drone hobby that allows me to create actual images I can use in my work as the API Evangelist. Having unique filtered images available for use in my writing is valuable to me--enough to justify the couple hundreds of dollars I spend each month on AWS servers. I know why I like applying image styles to my photos, but why do others? Most of the image filters out there we've seen from apps like Prisma are focused on fine art. Training image style transfer machine learning models on popular art that people are already familiar with. I guess this is allows people to apply the characteristics of art they like to the photographic layer of our increasingly digital lives. To me, it feels like some sort of art placebo. A way of superficially and algorithmic injecting what are brain tells us is artsy to our fairly empty, digital photo reality. Taking photos in real time isn't satisfying enough anymore. We need to distract ourselves from the world by applying reality to our digitally documented physical world--almost the opposite of augmented reality if there is such a thing. Getting lost in the ability to look at the real world through the algorithmic lens of our online life. We are stealing the essence the meaningful, tangible art from our real world, and digitizing it. We take this essense and algorithmically apply it our everyday life trying to add some color, some texture, but not too much. We need the photos to still be meaningful, and have context in our life, but we need to be able to spray an algorithmic lacquer of meaning on our intangible lives. The more filters we have, the more lenses we have to look at the exact same moment we live each day. We go to work. We go to school. We see the same scenery, the same people, and the same pictures each day. Now we are able to algorithmic shift, distort, and paint the picture of our lives we want to see. Now we can add color to our life. We are being trained to think we can change the palette, and are in control over our lives. We can colorize the old World War 2 era photos of our day, and choose whether we want to color within, or outside the lines. Our lives don't have to be just binary 1s and 0s, and black or white. Slowly, picture by picture, algorithmic transfer by algorithmic transfer, the way we see the world changes. We no longer settle for the way...]]></content>
    <id>http://apievangelist.com/2017/03/08/why-would-people-want-fine-art-trained-machine-learning-models/</id>
  </entry><entry>
    <title>Machine Learning Style Transfer For Museums, Libraries, and Collections</title>
    <link href="http://apievangelist.com/2017/03/07/machine-learning-style-transfer-for-museums-libraries-and-collections/"/>
    <updated>2017-03-07T14:00:00Z</updated>
    <content><![CDATA[I putting some thought into some next steps for my algorithmic rotoscope work, which is about the training and applying of image style transfer machine learning models. I'm talking with Jason Toy (@jtoy) over at&amp;nbsp;Somatic about the variety of use cases, and I want to spend some thinking about image style transfers, from the perspective of a collector or curator of images--brainstorming how they can organize, make available their work(s) for use in image style transfers. Ok, let's start with the basics--what am I talking about when I say image style transfer? &amp;nbsp;I recommend starting with a basic definition of machine learning in this context, providing by my girlfriend, and partner in crime Audrey Watters. Beyond, that I am just referring to the training a machine learning model by directing it to scan an image. This model can then be applied to other images, essentially transferring the style of one image, to any other image. There are a handful of mobile applications out there right now that let you apply a handful of filters to images taken with your mobile phone--Somatic is looking to be the wholesale provider of these features.&amp;nbsp; Training one of these models isn't cheap. It costs me about $20 per model in GPUs to create--this doesn't consider my time, just my hard compute costs (AWS bill). Not every model does anything interesting. Not all images, photos, and pieces of art translate&amp;nbsp;into cool features when applied to images. I've spent about $700 training 35 filters. Some of them are cool, and some of them are meh. I've had the most luck focusing on dystopian landscapes, which I can use in my storytelling around topics like immigration, technology, and the election.&amp;nbsp; This work ended up with Jason and I talking about museums and library collections, thinking about opportunities for them to think about their collections in terms of machine learning, and specifically algorithmic style transfer. Do you have images in your collection that would translate well for use in graphic design, print, and digital photo applications? I spend hours looking through art books for the right textures, colors and outlines. I also spend hours looking through graphic design archives for movie and gaming industry, as well as government collections. Looking for just the right set of images that will either transfer&amp;nbsp;and produce an interesting look, as well as possible transfer something meaningful to the new images that I am applying styles to. Sometimes style transfers just make a photo look cool, bringing some general colors, textures, and other features to a new photo--there really isn't any value in knowing what image was behind the style transfer, it just looks cool. Other times, the image can be enhanced knowing about the image behind the machine learning model, and not just transferring&amp;nbsp;styles between images, but also potentially transferring some meaning as well. You can see this in action when I took a nazi propaganda poster and applied to it to photo of Ellis Island, or I took an old Russian...]]></content>
    <id>http://apievangelist.com/2017/03/07/machine-learning-style-transfer-for-museums-libraries-and-collections/</id>
  </entry><entry>
    <title>The Russian Propaganda Distortion Field Around The White House</title>
    <link href="http://apievangelist.com/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/"/>
    <updated>2017-02-14T18:00:00Z</updated>
    <content><![CDATA[
I am having a difficult time reconciling what is going on with the White House right now. The distortion field around the administration right now feels like some bad acid trip from the 1980s, before I learned how to find the good LSD. After losing their shit over her emails and Benghazi, they are willing to overlook Russia fucking with our election on so many levels and infiltrating the White House. Wacky. Just fucking wacky!
The way Russia has fed the poor folk in this country a steady diet of bullshit is pretty crafty, as well as disturbing. Their approach to disinformation has dovetailed nicely with the approach of the GOP in this country. As usual, I am trying to understand and visualize the algorithmic distortion in this conversation, and how our current administration could be so heavily under the influence of Russian propaganda.
I'm going through Russian propaganda archives looking for the right colors and textures to shine a light on the algorithmic distortion raining down on the White House as part of this ongoing&amp;nbsp;Russian cyber attack. I'm using the posters I've found to train some machine learning models, and the first one has come off the cloud pipeline and was ready for applying to some images to see what the effect might be. I started with a couple photos I've taken of the White House. One from the lawn, and one from inside the Eisenhower Executive Office Building (EEOB) while I was working there.&amp;nbsp;
I like the results. It makes it look like the distortion field around the White House are just dense pamphlets raining down from above, dens like Internet packets aggregating&amp;nbsp;on a wireless&amp;nbsp;network fighting to get in. It's fascinating to watch people be so willfully ignorant to see the algorithmic distortion around them. Even with all the talk of wireless, mobile, the web, and cyber warfare. They don't see how they are under assault from information and disinformation--something the Russians seem to excel at.
There are 3 other posters being used to train machine learning models right now. I can only do one at a time and each one takes about 12 hours. Then it will take me about another week or so of applying them to images to find what works and doesn't work with the filters. I have about 40 individual filters currently, and I have been focusing heavily on dystopian textures in the previous round. I am thinking that this round I'll focus on colors and textures I can use to highlight the effects of the cyber on our reality -- I hear it is going to be huge.&amp;nbsp;
Here is the look from next door....

Here is the view from the front lawn...

]]></content>
    <id>http://apievangelist.com/2017/02/14/the-russian-propaganda-distortion-field-around-the-white-house/</id>
  </entry><entry>
    <title>Algorithmic Reflections On The Immigration Debate</title>
    <link href="http://apievangelist.com/2017/02/06/algorithmic-reflections-on-the-immigration-debate/"/>
    <updated>2017-02-06T13:00:00Z</updated>
    <content><![CDATA[We are increasingly looking through an algorithmic lens when it comes to politics in our everyday lives. I spend a significant portion of my days trying to understand how algorithms are being used to shift how we view and discuss politics. One of the ongoing themes in my research is focused on machine learning, which is an aspect of technology currently being applied to news curation, identifying fake news, all the way to how we monitor and see the world online with images and video.&amp;nbsp; Algorithms are painting a real-time picture that colors how we see the physical world around us--something that is increasingly occurring online for many of us. Because many of the creators of algorithms are white men, they often are blind and even willfully ignorant of how their algorithms and technological tools are used for evil purposes. With a focus on revenue and the interests of their investors, Twitter, Facebook, Reddit and other platforms often do not see (or are willing to turn a blind eye to) how hateful groups are using their platforms to spread misinformation and hate. When you combine this with a lack of awareness when it comes history, we end up in the current situation we find ourselves in with the Trump administration. As part of my work to understand how algorithms are shaping our world views I am playing with different ways of applying machine learning to my images and videos for use across my storytelling -- I am calling&amp;nbsp;@algorotoscope. It's helping me understand how machine learning works (or not), while also giving me an artistic distraction from the increasingly inhuman world of technology. Taking photos and videos, as well as the process of training and applying the filters gives me relief, allowing me to find some balance in the very toxic digital environment I find myself in today. I feel that we are allowing algorithms to amplify some very hateful views of the world right now, something that is being leveraged to produce some very damaging outcomes in the immigration debate. To help paint a picture of what I'm seeing from my vantage point, I took an old World War II nazi propaganda poster and used it to train a machine learning model, which I could then apply to any image or video using a platform called Algorithmia. Here is the resulting image.... The image is a photo I took from the waiting area at Ellis Island, with sunlight reflecting through the windows, lighting up the tiles in the room where millions of immigrants waiting to be admitted into this country. I feel like we are allowing our willful ignorance of history as Americans to paint the immigration debate today, something that is being accelerated and fueled by a small hateful portion of our society, with the assistance of algorithms. Facebook, Twitter, Reddit, and other platforms are allowing their algorithms to be gamed by this very vocal minority in a way that is shaping the views of the larger population--making for...]]></content>
    <id>http://apievangelist.com/2017/02/06/algorithmic-reflections-on-the-immigration-debate/</id>
  </entry><entry>
    <title>When The Companies Who Have All Your Digital Bits Promise Not To Recreate You</title>
    <link href="http://apievangelist.com/2017/01/14/when-the-companies-who-have-all-your-digital-bits-promise-not-to-recreate-you/"/>
    <updated>2017-01-14T17:00:00Z</updated>
    <content><![CDATA[I'm thinking about my digital bits a lot lately. Thinking about the digital bits that I create, the bits I generate automatically, the bits I own, the bits I do not own, and how I can make a living with just a handful of my bits. I have an inbox full of people who want me to put my bits on their websites, and people who want to put their bits on my platform&amp;nbsp;so that they are associated with my brand, increasing the value of their bits. I know people think I'm crazy (I am) when I talk so much about my bits in this way, but it is a just response to my front-row seat watching companies getting pretty wealthy off all of our bits. #BlueManGroup Obviously, this is not a new phenomenon, and we've heard stories about Prince, John Fogerty, and George Clinton fighting for the funk and ownership of their musical bits, something artists of all types have had to battle on all fronts, throughout their careers. Lately, I have I have found myself sucked in listening to stories from Carrie Fisher in her documentaries, better understanding her struggles to maintain a voice in the merchandising, representation and control over her likeness, and her most famous role--Princess Leia. &amp;lt;3 Carrie Fisher made Prince Leia the icon she is today. However, she did it on the LucasFilm platform. How much does LucasFilm own, and how much does Carrie Fisher own? How dependent are they on her, and how dependent is she on them. Something that has been intensely&amp;nbsp;worked out between lawyers since the 1970's. Now that she has passed, I'm sure her estate will continue to take on LucasFilm on this front, but the company has so many of her video, audio, and images (her bits), that they can possibly recreate her for future movies if they desired. As I'm thinking about my own bits, and the control, or lack of control I have over these this week, I'm also reading that&amp;nbsp;Lucasfilm released a statement that: We want to assure our fans that Lucasfilm has no plans to digitally recreate Carrie Fisher&amp;rsquo;s performance as Princess or General Leia Organa. Remember the Tupac and Michael Jackson holograms? The precedent for digitally recreate all of&amp;nbsp;or the parts of pieces (bits) of a human is&amp;nbsp;out there. Let me stop here. I'm not talking about anything remotely in realm of the singularity, I'm simply talking about what is possible with existing technology using video, audio, images, and text content generated from or containing the fingerprint of a certain human being (me). I know that some geeks love to masturbate to this shit, but I'm just talking about some of you delusion mother-fuckers realizing there is a lot of money to be off of someone else's hard work, or even just their human existence. #Exploitatification The platformification of everything is all about getting people to come do shit on your platform, and making money doing this--I just happen to study this stuff for...]]></content>
    <id>http://apievangelist.com/2017/01/14/when-the-companies-who-have-all-your-digital-bits-promise-not-to-recreate-you/</id>
  </entry><entry>
    <title>Algorithmia&#039;s Multi-Platform Data Storage Solution For Machine Learning Workflows</title>
    <link href="http://apievangelist.com/2017/01/06/algorithmias-multiplatform-data-storage-solution-for-machine-learning-workflows/"/>
    <updated>2017-01-06T18:00:00Z</updated>
    <content><![CDATA[I've been working with Algorithmia to manage a large number of images as part of&amp;nbsp;my algorithmic rotoscope side project, and they have a really nice omni-platform approach to allowing me to manage my images and other files I am using in my machine learning workflows. Images, files, and the input and output of heavy object is an essential part of almost any machine learning task, and Algorithmia makes easy to do across the storage platforms we use the most (hopefully).&amp;nbsp; Algorithmia provides you with local data storage--pretty standard stuff, but they also allow you to connect your Amazon S3 account, or your Dropbox account, and connect to specific folders, buckets, while helping you handle all of your permissions. Maybe I have my blinders on with this because I heavily use Amazon S3 as me default online storage, and Dropbox is my secondary store, but I think the concept still is worth sharing.. This allows me to seamlessly manage the objects, documents, files, and other images I store across my operation as part of my machine learning workflow. &amp;nbsp;Algorithmia even provides you with an intuitive way of referencing files, by allowing each Data URI to uniquely identifies files and directories, with each composed of a protocol and a path, with each service having its own unique protocol: data:// Algorithmia hosted data dropbox:// Dropbox default connected accounts S3:// Amazon S3 default connected account This approach dramatically simplifies my operations when working with files, and allows me to leverage the API driven storage services I am already putting to work, while also taking advantage of the growing number of algorithms available to me in Algorithmia's catalog. In my algorithmic rotoscope project I am breaking videos into individual images, producing 60 images per second of video, and uploading to Amazon S3. Once images are uploaded, I can then run Algorithmia's Deep Filter algorithm against all images, sometimes thousands of images, using their text models, or any of the 25+ I've trained myself.&amp;nbsp; This approach is not limited to just video and images, this is generic to any sort of API driven machine learning orchestration. Just swap out video and images, with mapping, content, or other resource, and then find the relevant machine learning workflow you need to apply, and get to work. While I am having fun playing with my drone videos and texture filters, the approach can be just as easily applied to streamline any sort of marchine learning workflow. One additional benefit of storing data this way is I've found Dropbox to be a really amazing layer for including humans in the workflow. I leverage Amazon S3 for my wholesale, compute grade storage, but Dropbox is where I publish images, videos, and documents that I need to put in front of humans, or include them in the machine learning workflow. I find this gives them a role in the process, in a way that gives them control over the data, images, videos, and other objects, on a platform they are already comfortable...]]></content>
    <id>http://apievangelist.com/2017/01/06/algorithmias-multiplatform-data-storage-solution-for-machine-learning-workflows/</id>
  </entry><entry>
    <title>Finding The Right Dystopian Filter To Represent The World Unfolding Around Us</title>
    <link href="http://apievangelist.com/2017/01/03/finding-the-right-dystopian-filter-to-represent-the-world-unfolding-around-us/"/>
    <updated>2017-01-03T15:00:00Z</updated>
    <content><![CDATA[I got sucked into a project over the holidays, partly because it was an interesting technical challenge, but mostly because it provided me with a creative distraction after the election. I started playing with image filters from Algorithmia, using their Deep Filter service, which some may recognize as being similar to services like Prisma. The difference is with Algorithmia is you can use their 30+ filters, or if you want you can train your own image filters using their AWS machine learning AMI. As I was playing with Algorithmia after the election, I had many images in my head of the dystopian landscape that is unfolding around us. Many of these images were reminiscent of my childhood in the 70's and 80's, during the cold war, where the future perpetually seemed very bleak to me. I wanted a way to take these images from my head and apply to the photos I was taking, and even better, what if I could to it to the video, and more specifically, the drone videos I am making. Four weeks later, I have gotten to the first set of filters, that when applied to my photography that gets me closer to the visions I had in my head. Here is an original photo taken&amp;nbsp;by me on January 2nd, 2017 in East Los Angeles: Next, I wanted to reduce the world around us to be less than real, comic, or drawn. I wanted a way to algorithmically reduce the outlines of the world into something that resembled our real world, to make things as familiar as possible, but then quickly bending and skewing it, so that I could help us see how dark things are becoming. To borrow a phrase from my partner in crime, I wanted to be able to reduce everything I captured in my photography and videos down to a transaction. I wanted to show us how the world around us is being digitized, de-humanized, and rendered into an even more hostile landscape, that has very little concern for the humans living in it. I wanted to be able to go even further and visualize how noisy the world has become, not because of cars and airplanes, but because of our bits and bytes that were flowing around us every day. Help us visualize the constant assault on us, the people we love, and that increasingly there is no escape from this constant assault--it is in our homes, cars, business, and public spaces. I want to paint a dark dystopian digital landscape, but ultimately I want as wide as a possible palette as I can. I needed an algorithmic palette of colors and textures that were born from the true artists who came before us, making the colors and textures familiar, and even soft before I took things to a much darker level. I didn't want to just shock, I wanted to slowly shift the world around us down a dystopian road. Transforming&amp;nbsp;our world into a cartoon or painting in a...]]></content>
    <id>http://apievangelist.com/2017/01/03/finding-the-right-dystopian-filter-to-represent-the-world-unfolding-around-us/</id>
  </entry><entry>
    <title>Exploring The Economics of Wholesale and Retail Algorithmic APIs</title>
    <link href="http://apievangelist.com/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/"/>
    <updated>2017-01-03T15:00:00Z</updated>
    <content><![CDATA[I got sucked into a month long project applying machine learning filters to video over the holidays. The project began with me doing the research on the economics behind Algorithmia's machine learning services, specifically the DeepFilter algorithm in their catalog. My algorithmic rotoscope work applying Algorithmia's Deep Filters to images and drone videos has given me a hands-on&amp;nbsp;view of Algorithmia's approach to algorithms, and APIs, and the opportunity to think pretty deeply about the economics of all of this. I think Algorithmia's vision of all of this has a lot of potential for not just image filters, but any sort of algorithmic and machine learning API. Retail Algorithmic and Machine Learning APIsUsing Algorithmia is pretty straightforward. With their API or CLI you can make calls to a variety of algorithms in their catalog, in this case their DeepFilter solution. All I do is pass them the URL of an image, what I want the new filtered image to be called, and the name of the filter that I want to be applied. Algorithmia provides an API explorer you can copy &amp;amp; paste the required JSON into, or they also provide a demo application for you to use--no JSON required.&amp;nbsp; Training Your Own Style Transfer Models Using Their AWS AMIThe first &quot;rabbit hole&quot; concept I fell into when doing the research on Algorithmia's model was their story on creating your own style transfer models, providing you step by step details on how to train them, including a ready to go AWS AMI that you can run as a GPU instance. At first, I thought they were just cannibalizing their own service, but then I realized it was much more savvier than that. They were offloading much of the costly compute resources needed to create the models, but the end product still resulted in using their Deep Filter APIs.&amp;nbsp; Developing My Own API Layer For Working With Images and VideosOnce I had experience using Algorithmia's deep filter via their API, and had produced a handful of my own style transfer models, I got to work designing my own process for uploading and applying the filters to images, then eventually separating out videos into individual images, applying the filters, then reassembling them into videos. The entire process, start to finish is a set of APIs, with a couple of them simply acting as a facade for Algorithmia's file upload, download, and DeepFilter APIs. It provided me with a perfect hypothetical business for thinking through the economics of building on top of Algorithmia's platform. Defining My Hard Costs of Algorithmia's Service and the AWS Compute NeededAlgorithmia provides a pricing calculator along with each of their algorithms, allowing you to easily predict your costs. They charge you per API call, and the compute usage by the second. Each API has its own calculator, and average runtime duration costs, so I'm easily able to calculate a per image cost to apply filters--something that exponentially grows when you are applying to 60 frames (images) per second of...]]></content>
    <id>http://apievangelist.com/2017/01/03/exploring-the-economics-of-wholesale-and-retail-algorithmic-apis/</id>
  </entry><entry>
    <title>Learning About Machine Learning APIs With My Algorithmic Rotoscope Work</title>
    <link href="http://apievangelist.com/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/"/>
    <updated>2017-01-03T12:00:00Z</updated>
    <content><![CDATA[I was playing around with Algorithmia for a story about their business model back in December, when I got sucked into playing with their DeepFilter service, resulting in a 4-week long distraction which ultimately became what I am calling my&amp;nbsp;algorithmic rotoscope&amp;nbsp;work. After weeks of playing around, I have a good grasp of what it takes to separate videos into individual images, applying the&amp;nbsp;Algorithmia&amp;nbsp;machine learning filters, and reassembling them as videos. I also have several of my own texture filters created now using the AWS AMI and process provided&amp;nbsp;Algorithmia--you can learn more about algorithmic rotoscope, and details of what I did via the Github project updatese. The project has been a great distraction from what I should be doing. After the election, I just did not feel like doing my regular writing, scheduling of Tweets, processing of press releases, and the other things I do on a regular basis. Algorithmic Rotoscope provided a creative, yet a&amp;nbsp;still very API focused project to take my mind off things during the holidays. It was a concept I couldn't get out of my head, which is always a sign for me that I should be working on a project. The work was more involved than I anticipated, but after a couple weeks of tinkering, I have the core process for applying filters to videos working well, allowing me to easily apply the algorithmic textures. Other than just being a distraction, this project has been a great learning experience for me,&amp;nbsp;with several aspects keeping me engaged: Algorithmia's Image Filters&amp;nbsp; - Their very cool DeepFilter service, which allows you to apply artistically and stylish filters to your images using their API or CLI, providing over 30 filters you can use right away. Training Style Transfer Models - Firing up an Amazon GPU instance, look through art books and find interesting pieces that can be used to train the machine learning models, so you can define your own filters. Applying Filters To Images - I spent hours playing with Algorithmia's filters, applying to my photo library, experimenting, and playing around with what looks good, and what is possible. Applying Filters To Videos - Applying Algorithmia's, and my own filters video I have laying around, especially what is possible when applied to the GB's of drone video I have laying around, something that is only going to grow. Why is this an API story? Well, first of all, it uses the Algorithmia API, but I also developed the separation of the videos, applying filters to images, and reassembling the videos as an API. It isn't anything that is production stable, but I've processed thousands of images, many minutes of video, and made over 100K API calls to Algorithmia. Next, I am going to write-up Algorithmia's business model, using my algorithmic rotoscope work as a hypothetical API-driven business--helping me think through the economics of building a SaaS or retail API solution on top of Algorithmia.&amp;nbsp; Beyond being an API story, it has been a lot of fun to engineer, and...]]></content>
    <id>http://apievangelist.com/2017/01/03/learning-about-machine-learning-apis-with-my-algorithmic-rotoscope-work/</id>
  </entry><entry>
    <title>Make It An API Driven Publishing Solution</title>
    <link href="http://apievangelist.com/2016/12/17/make-it-an-api-driven-publishing-solution/"/>
    <updated>2016-12-17T00:48:00Z</updated>
    <content><![CDATA[Once I had established a sort of proof of concept for my algorithmic rotoscope process, and was able to manually execute each step of the process from separating a video, and applying filters, to reassembling the video, I quickly refactored my prototype code to be API-first. I did this even before I built any sort of interface for executing and managing the process, as this would allow me to not just execute the process, it would also allow me to manage, extend, and scale as many algorithmic rotoscopes as I wanted.
I'm not particularly proud of the API design, and think it is something that will evolve and change, as I push forward what is possible with my algorithmic rotoscope. I'm learning a lot along the way, and my focus in having an API is not to open up access to 3rd parties, but to allow me to scale my process, and efficiently run using Amazon Web Services, Algorithmia, and a handful of other APIs. Currently, I have about 25 separate paths for my API, which allows me to accomplish every step of the algorithmic rotoscope process. 
Currenlty my algorithmic rotoscope API runs on a single Amazon EC2 instance, which I am scaling vertically, meaning I just increase the size of the server instance when I want more to get done. However, having the entire process be API first, will allow me to easy scale horizontally, across multiple servers. This should allow me to isolate specific steps of the process, or several of them together, allowing me to scale the server separately for assembling and disassembling the videos--which can be pretty intensive.
Once I have some time I will publish an OpenAPI Spec for the API. I don't have any intention on opening up the API for 3rd party usage, but I may be open to deploying Amazon instances for wholesale use, and partner access, at some point in the future. I am not looking to do this as a full blown startup idea, but is something I'd like to evolve on the side for my own uses, and potentially as a service for select clients. I will keep adding paths to my algorithmic rotoscope API, but for now I'll work to document, and refine the existing paths I have, and tell the story of the API--you know, practice what I preach and all. Even if my API isn't fully public, there is no reason the documentation, story, and results can't be public. 
]]></content>
    <id>http://apievangelist.com/2016/12/17/make-it-an-api-driven-publishing-solution/</id>
  </entry><entry>
    <title>Unique Algorithmic Filters Is Where It Is At</title>
    <link href="http://apievangelist.com/2016/12/16/unique-algorithmic-filters-is-where-it-is-at/"/>
    <updated>2016-12-16T00:48:00Z</updated>
    <content><![CDATA[I am having a blast with the image texture filters that Algorithmia includes as part of their DeepLearning service. I've been playing with how each filter will behave when applied to different images. Some work better for the desert images, while others work best for water, and I'll apply to my waterfalls, lakes, and rivers. The process has been a welcome distraction, but where I really get lost thinking about the possibilities is when it comes to creating your own filters--an area I am just getting started with. 
Algorithmia provides a pretty straightforward guide to creating your own filters. Once you fire up the EC2 GPU instance, and follow their setup process, creating filters is pretty easy, but is also pretty addictive, depending on what kind of habit can afford. ;-) I have created six filters so far, but plan on creating more as soon as I get some money and more time. Just like applying the filters, training filters takes some practice, and experience training it against a variety of images, colors, textures, etc.
Experience applying filters to a variety of images is important and valuable, but experience training and creating filters I think is where it is at. Being able to find just the right filter to apply to an image or images used in the video is valuable, but being able to identify and create and train exactly the right set of textures, colors, and filters could provide some really unique experiences. I'm not a big fan of the concept of intellectual property, but I could see knowledge of training your texture and filter algorithms against specific pieces of art, photographs, and elements from our physical worlds being a pretty potentially unique offering--something you'd want to keep secret.
Some of the Algorithmia filters are loud and intense, which I like for some applications, but I'm finding their lighter touch, more artistic, and subtle filters have a wider range of uses. I'm applying these findings to the filters I'm training, but I need more experience applying existing filters, as well as the training of new filters. All of this takes a significant amount of compute and storage power--which costs money. I have made my algorithmic rotoscope framework API-centric so that I can scale this and increase the number of videos I am able to process, as well as the number of filters I am able to create and add to the process.
I am going to create around 10-15 more filters, then spend time just applying to see what I can produce. Then I'm hoping to have enough experience in applying and training to know what works best, what I like, and what compliments my approach to drone video capture. Eventually, I am hoping to establish my own unique set of filters and my own unique style in applying them to video using my algorithmic rotoscope process. 
]]></content>
    <id>http://apievangelist.com/2016/12/16/unique-algorithmic-filters-is-where-it-is-at/</id>
  </entry><entry>
    <title>Opportunity In Breaking Up Videos Into Separate Images</title>
    <link href="http://apievangelist.com/2016/12/15/opportunity-in-breaking-up-videos-into-separate-images/"/>
    <updated>2016-12-15T00:48:00Z</updated>
    <content><![CDATA[I have been working my way through about 300 GB of drone and GoPro videos from this summer. One of the lingering thoughts I was having throughout this process centered around the concept of breaking videos up into individual slides. Once I stumbled across Algorithmia's Deep Learning, I found myself thinking about how I could break up videos into separate images, and apply algorithmic texture filters to each individual image, and then reassemble back into a video.
FFMPEG stood out as the solution I needed to accomplish this. It was pretty easy to export each video as separate .jpg files, as well as the ability to assemble any images into a video. What did take me some time was getting the frame rate, size, and other finer aspects of video to work as I desired, and in sync with my drone videos. To achieve an acceptable video I needed 60 individual images for each second of video, potentially making the work a compute and storage intensive endeavor.
Using Amazon EC2 and S3 it isn't too much work to leverage FFMPEG to break up videos into separate images. Once I did this, I started publishing a JSON representation of each video, along with each individual image to Github, leverage version control, forking, and the other benefits of the platform, but for use in managing a video, instead of code. This process has given each of my videos, a machine readable, Git managed the representation of each video for me to work with, manipulate, and evolve independently. 
When you combine the separation of video, with Github in this way, with an API-driven approach to separation, assembly, as well as individual and overall image manipulation, the possibilities are pretty limitless. After I separate out each video into images, I am then applying textures and filters to each individual image using Algorithmia's deep filters, a process that has numerous opportunities, but I think the wider approach holds more potential than just fun filters. This approach is the primary reason I did this work. Playing with the videos, filters, and images was a welcome distraction, but I think the opportunity around breaking up videos into separate images is much bigger than algorithmic rotoscope.
]]></content>
    <id>http://apievangelist.com/2016/12/15/opportunity-in-breaking-up-videos-into-separate-images/</id>
  </entry><entry>
    <title>Algorithmic Rotoscope</title>
    <link href="http://apievangelist.com/2016/12/05/algorithmic-rotoscope/"/>
    <updated>2016-12-05T03:09:08Z</updated>
    <content><![CDATA[I had come across Texture Networks: Feed-forward Synthesis of Textures and Stylized Image from Cornell University a while back in my regular monitoring of the API space, so I was pleased to see Algorithmia building on this work with their Deep Filter project. The artistic styles and textures you can apply to images is fun to play with, and even more fun when you apply at scale using their API. While this was fun, what really caught my attention was their post on their open source AWS AMI for training style transfer models--where you can develop your own image filter. I fired up an AWS instance, and within 48 hours I had my first image filter. For some reason, I went to bed that night thinking about drone video footage and began wondering if I could apply Algorithmia's filters, as well as the custom filters I create to drone footage. The next day I started playing with my prototype, to see what was possible with applying this type of image filtering to videos. My approach has three distinct features: Separate Video Into Images - Using FFMPEG, I created a function that would take any video I gave it and separate it into an image for each second of the video, and write them all to a folder. Apply Filters to Each Image - After uploading all the images for each video to Algorithm.io using their API, I would select one of their 30+ image filters or any of the filters I created. Reassemble Video From Images - Once I've applied one of the algorithmic filters to all of the images I reassemble them back into a video. While the videos I've been able to produce so far are interesting enough, I am intrigued by the process of defining and applying the filters. There are a handful of things going on here for me: Defining Of The Algorithms Behind Each Image Filter - I'm having a blast with the image filters that Algorithmia provide, but their work got me thinking about how I can train image filters specifically for being applied to video in this way. I'm learning a lot of about the training process--something I want to keep working on. Applying Algorithmic Filters To Images At Scale (Videos) - The videos I am working with currently have been 500 and 2000 individual images after I separate the video. I am learning a lot about working with video at the individual slide level, and have a lot more work ahead of me. Changing How I Take Videos Using My Drone - I have been applying this work to 4K video I either took myself or was present when it was taken. As I'm applying these filters I am hyper aware of where the sun was positioned, how the shadows of rocks, trees, and buildings play in, making think deeply about how I can design future drone shots. Algorithmia has enough filters to keep me interested for a while, but this is...]]></content>
    <id>http://apievangelist.com/2016/12/05/algorithmic-rotoscope/</id>
  </entry>
</feed>
